{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import jieba\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchtext import data\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# hyper parameters\n",
    "STR_MAXLEN = 30\n",
    "BATCH_SIZE = 256\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBED_DIM = 200\n",
    "HIDDEN_DIM = 100\n",
    "ATTEN_UNIT = 350\n",
    "ATTEN_HOPS = 4\n",
    "DEEP_LAYERS = [200]\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHES = 20\n",
    "DECAY_STEP = 2\n",
    "DECAY_GAMMA = 0.99\n",
    "CLASS_WEIGHT = [0.6116, 2.7397]\n",
    "def print_flush(data, args=None):\n",
    "    if args == None:\n",
    "        print(data)\n",
    "    else:\n",
    "        print(data, args)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def pad_seq(seq, max_length):\n",
    "    length = len(seq[0])\n",
    "    pad_leng = 0 if length > max_length else (max_length-length)\n",
    "    if pad_leng == 0:\n",
    "        seq = seq[:, :max_length]\n",
    "    else:\n",
    "        seq = torch.cat([seq, torch.ones(len(seq), pad_leng).long().to(DEVICE)], dim=1)\n",
    "    return seq\n",
    "\n",
    "def Frobenius(mat):\n",
    "    size = mat.size()\n",
    "    if len(size) == 3:  # batched matrix\n",
    "        ret = (torch.sum(torch.sum((mat ** 2), 2), 1).squeeze() + 1e-10) ** 0.5\n",
    "        return torch.sum(ret) / size[0]\n",
    "    else:\n",
    "        raise Exception('matrix for computing Frobenius norm should be with 3 dims')\n",
    "        \n",
    "def getIdentityMatrix(batch_size, attention_hops):\n",
    "    I = torch.zeros(batch_size, attention_hops, attention_hops)\n",
    "    for i in range(batch_size):\n",
    "        for j in range(attention_hops):\n",
    "            I.data[i][j][j] = 1\n",
    "    return I.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process raw data...\n",
      "Building vocabulary Finished.\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [txt for txt in text]\n",
    "class BatchWrapper:\n",
    "    def __init__(self, dl, iter_columns):\n",
    "        self.dl, self.iter_columns = dl, iter_columns  # we pass in the list of attributes for x &amp;amp;amp;amp;lt;g class=\"gr_ gr_3178 gr-alert gr_spell gr_inline_cards gr_disable_anim_appear ContextualSpelling ins-del\" id=\"3178\" data-gr-id=\"3178\"&amp;amp;amp;amp;gt;and y&amp;amp;amp;amp;lt;/g&amp;amp;amp;amp;gt;\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield (getattr(batch, attr) for attr in self.iter_columns)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "print_flush('process raw data...')\n",
    "TEXT = data.Field(sequential=True, use_vocab=True, eos_token='<EOS>', init_token='<BOS>',pad_token='<PAD>', \n",
    "                  batch_first=True, tokenize=tokenizer)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "\n",
    "tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"txt1\", TEXT), (\"txt2\", TEXT),\n",
    "                 (\"label\", LABEL)]\n",
    "\n",
    "train = data.TabularDataset(path='../datasets/train.csv', format='csv', skip_header=True, fields=tv_datafields)\n",
    "valid = data.TabularDataset(path='../datasets/valid.csv', format='csv', skip_header=True, fields=tv_datafields)\n",
    "\n",
    "TEXT.build_vocab(train, valid, min_freq=3)\n",
    "print_flush('Building vocabulary Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1517"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data done!\n"
     ]
    }
   ],
   "source": [
    "train_iter = data.BucketIterator(dataset=train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.txt1) + len(x.txt2), shuffle=True, device=DEVICE, repeat=False)\n",
    "valid_iter = data.Iterator(dataset=valid, batch_size=BATCH_SIZE, device=DEVICE, shuffle=False, repeat=False)\n",
    "\n",
    "train_dl = BatchWrapper(train_iter, [\"txt1\", \"txt2\", \"label\"])\n",
    "valid_dl = BatchWrapper(valid_iter, [\"txt1\", \"txt2\", \"label\"])\n",
    "\n",
    "print_flush('prepare data done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentiveEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, atten_unit, atten_hops):\n",
    "        super(SelfAttentiveEncoder, self).__init__()\n",
    "#         self.dropout = nn.Dropout(config['dropout'])\n",
    "        self.ws1 = nn.Linear(hidden_dim * 2, atten_unit, bias=False)\n",
    "        self.ws2 = nn.Linear(atten_unit, atten_hops, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.attention_hops = atten_hops\n",
    "\n",
    "    def init_weights(self, init_range=0.1):\n",
    "        self.ws1.weight.data.uniform_(-init_range, init_range)\n",
    "        self.ws2.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, hidden_txt1):\n",
    "        size1 = hidden_txt1.size()\n",
    "#         size2 = hidden_txt2.size()\n",
    "        hidden_txt1_ = hidden_txt1.contiguous().view(-1, size1[2])  # [bsz*seq_len, hidden_dim]\n",
    "#         hidden_txt2_ = hidden_txt2.contiguous().view(-1, size2[2])  # [bsz*seq_len, hidden_dim]\n",
    "\n",
    "        hbar1 = self.tanh(self.ws1(hidden_txt1_))  # [bsz*seq_len, attention-units]\n",
    "#         hbar2 = self.tanh(self.ws1(hidden_txt2_))  # [bsz*seq_len, attention-units]\n",
    "\n",
    "        alphas1 = self.ws2(hbar1).view(size1[0], size1[1], -1)  # [bsz, seq_len, hops]\n",
    "        alphas1 = torch.transpose(alphas1, 1, 2).contiguous()  # [bsz, hops, seq_len]\n",
    "\n",
    "#         alphas2 = self.ws2(hbar2).view(size2[0], size2[1], -1)  # [bsz, seq_len, hops]\n",
    "#         alphas2 = torch.transpose(alphas2, 1, 2).contiguous()  # [bsz, hops, seq_len]\n",
    "        alphas1 = self.softmax(alphas1.view(-1, size1[1]))  # [bsz*hop, seq_len]\n",
    "#         alphas2 = self.softmax(alphas2.view(-1, size2[1]))  # [bsz*hop, seq_len]\n",
    "\n",
    "        alphas1 = alphas1.view(size1[0], self.attention_hops, size1[1])  # [bsz, hop, seq_len]\n",
    "#         alphas2 = alphas2.view(size2[0], self.attention_hops, size2[1])  # [bsz, hop, seq_len]\n",
    "#         print('alphas1', alphas1)\n",
    "#         print('alphas1', alphas1.size())\n",
    "#         print('hidden_txt1', hidden_txt1)\n",
    "#         print('hidden_txt1', hidden_txt1.size())\n",
    "\n",
    "        return torch.bmm(alphas1, hidden_txt1), alphas1\n",
    "\n",
    "#     def init_hidden(self, bsz):\n",
    "#         return self.bilstm.init_hidden(bsz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class baseline_LSTM_self_attention(nn.Module):\n",
    "    def __init__(self, vocab, embed_dim, hidden_dim, atten_unit, atten_hops, deep_layers):\n",
    "        super(baseline_LSTM_self_attention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.deep_layers = deep_layers\n",
    "        self.atten_unit = atten_unit\n",
    "        self.atten_hops = atten_hops\n",
    "        self.word_embed = nn.Embedding(len(vocab), embed_dim, padding_idx=vocab['<PAD>'])\n",
    "        self.lstm_embed = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.self_attention1 = SelfAttentiveEncoder(hidden_dim, atten_unit,atten_hops)\n",
    "        self.self_attention2 = SelfAttentiveEncoder(hidden_dim, atten_unit,atten_hops)\n",
    "        self.deep_layer_1 = nn.Linear(4 * hidden_dim*2, deep_layers[0])\n",
    "        self.deep_out = nn.Linear(deep_layers[0], 2)\n",
    "#         self.deep_dropout_1 = nn.Dropout(0.5)\n",
    "    def forward(self, txt1, txt2, hidden=None):\n",
    "        embed_txt1 = self.word_embed(txt1)\n",
    "        embed_txt2 = self.word_embed(txt2)\n",
    "#         lstm_txt1 = torch.sum(self.lstm_embed(embed_txt1, hidden)[0], 1)\n",
    "#         lstm_txt2 = torch.sum(self.lstm_embed(embed_txt2, hidden)[0], 1)\n",
    "        lstm_txt1 = self.lstm_embed(embed_txt1, hidden)[0]\n",
    "        lstm_txt2 = self.lstm_embed(embed_txt2, hidden)[0]\n",
    "        atten_txt1, mat1 = self.self_attention1(lstm_txt1)\n",
    "        atten_txt2, mat2 = self.self_attention2(lstm_txt2)\n",
    "\n",
    "        atten_txt1 = atten_txt1.unsqueeze_(1)\n",
    "        atten_txt2 = atten_txt2.unsqueeze_(1)\n",
    "        atten_txt1 = F.max_pool2d(atten_txt1, (self.atten_hops, 1)) \n",
    "        atten_txt2 = F.max_pool2d(atten_txt2, (self.atten_hops, 1)) \n",
    "        atten_txt1 = atten_txt1.squeeze()\n",
    "        atten_txt2 = atten_txt2.squeeze()\n",
    "#         print('atten_txt1', atten_txt1)\n",
    "#         print('atten_txt1', atten_txt1.size())\n",
    "#         print('atten_txt2', atten_txt2)\n",
    "#         print('atten_txt2', atten_txt2.size())\n",
    "        txt_substract = torch.abs(atten_txt1 - atten_txt2)\n",
    "        txt_multiply = torch.mul(atten_txt1, atten_txt2)\n",
    "        output = self.deep_layer_1(torch.cat([atten_txt1, atten_txt2, txt_substract, txt_multiply], 1))\n",
    "        output = self.deep_out(output)\n",
    "        return F.log_softmax(output), mat1, mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_LSTM_self_attention(TEXT.vocab.stoi, EMBED_DIM, HIDDEN_DIM, ATTEN_UNIT, ATTEN_HOPS, DEEP_LAYERS)\n",
    "model.to(DEVICE)\n",
    "criterion = nn.NLLLoss(weight=torch.tensor(CLASS_WEIGHT).float().to(DEVICE))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = StepLR(optimizer, step_size=DECAY_STEP, gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train...\n",
      "batch number 337 \n",
      "learning rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 50] loss: 3.474555 metric: 0.336634 time: 1.6 s\n",
      "[1 100] loss: 2.637081 metric: 0.268293 time: 1.3 s\n",
      "[1 150] loss: 2.137220 metric: 0.355556 time: 1.4 s\n",
      "[1 200] loss: 0.764659 metric: 0.322917 time: 1.4 s\n",
      "[1 250] loss: 0.694769 metric: 0.363636 time: 1.5 s\n",
      "[1 300] loss: 0.671105 metric: 0.470588 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 1]. loss: 0.608093 acc: 0.658718 f1: 0.420704 time: 10.6 s\n",
      "**************************************************\n",
      "save model...\n",
      "learning rate: 0.001000\n",
      "[2 50] loss: 0.639672 metric: 0.444444 time: 1.5 s\n",
      "[2 100] loss: 0.609722 metric: 0.376623 time: 1.4 s\n",
      "[2 150] loss: 0.602120 metric: 0.535032 time: 1.5 s\n",
      "[2 200] loss: 0.591416 metric: 0.426230 time: 1.4 s\n",
      "[2 250] loss: 0.589789 metric: 0.437500 time: 1.4 s\n",
      "[2 300] loss: 0.584310 metric: 0.419355 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 2]. loss: 0.581097 acc: 0.742087 f1: 0.471044 time: 10.6 s\n",
      "**************************************************\n",
      "save model...\n",
      "learning rate: 0.001000\n",
      "[3 50] loss: 0.531307 metric: 0.606061 time: 1.5 s\n",
      "[3 100] loss: 0.541671 metric: 0.500000 time: 1.4 s\n",
      "[3 150] loss: 0.542903 metric: 0.478261 time: 1.4 s\n",
      "[3 200] loss: 0.538006 metric: 0.523077 time: 1.4 s\n",
      "[3 250] loss: 0.539341 metric: 0.565217 time: 1.4 s\n",
      "[3 300] loss: 0.535787 metric: 0.577465 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 3]. loss: 0.557442 acc: 0.695737 f1: 0.475890 time: 10.6 s\n",
      "**************************************************\n",
      "save model...\n",
      "learning rate: 0.000990\n",
      "[4 50] loss: 0.484061 metric: 0.531250 time: 1.5 s\n",
      "[4 100] loss: 0.483847 metric: 0.524823 time: 1.3 s\n",
      "[4 150] loss: 0.482194 metric: 0.475248 time: 1.4 s\n",
      "[4 200] loss: 0.509349 metric: 0.542373 time: 1.4 s\n",
      "[4 250] loss: 0.488091 metric: 0.619048 time: 1.5 s\n",
      "[4 300] loss: 0.491357 metric: 0.560000 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 4]. loss: 0.548548 acc: 0.688236 f1: 0.476980 time: 10.4 s\n",
      "**************************************************\n",
      "save model...\n",
      "learning rate: 0.000990\n",
      "[5 50] loss: 0.432182 metric: 0.638655 time: 1.5 s\n",
      "[5 100] loss: 0.441309 metric: 0.611570 time: 1.5 s\n",
      "[5 150] loss: 0.448975 metric: 0.549020 time: 1.5 s\n",
      "[5 200] loss: 0.442320 metric: 0.545455 time: 1.4 s\n",
      "[5 250] loss: 0.457413 metric: 0.600000 time: 1.4 s\n",
      "[5 300] loss: 0.455843 metric: 0.530864 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 5]. loss: 0.604962 acc: 0.788254 f1: 0.505272 time: 10.7 s\n",
      "**************************************************\n",
      "save model...\n",
      "learning rate: 0.000980\n",
      "[6 50] loss: 0.392482 metric: 0.590164 time: 1.6 s\n",
      "[6 100] loss: 0.388399 metric: 0.576923 time: 1.4 s\n",
      "[6 150] loss: 0.406570 metric: 0.616541 time: 1.4 s\n",
      "[6 200] loss: 0.415918 metric: 0.608000 time: 1.5 s\n",
      "[6 250] loss: 0.412046 metric: 0.651852 time: 1.5 s\n",
      "[6 300] loss: 0.419092 metric: 0.661017 time: 1.5 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 6]. loss: 0.580563 acc: 0.744465 f1: 0.501072 time: 10.7 s\n",
      "**************************************************\n",
      "learning rate: 0.000980\n",
      "[7 50] loss: 0.344412 metric: 0.730769 time: 1.5 s\n",
      "[7 100] loss: 0.358508 metric: 0.765957 time: 1.4 s\n",
      "[7 150] loss: 0.375111 metric: 0.645161 time: 1.5 s\n",
      "[7 200] loss: 0.367895 metric: 0.649682 time: 1.4 s\n",
      "[7 250] loss: 0.366143 metric: 0.551724 time: 1.5 s\n",
      "[7 300] loss: 0.376782 metric: 0.643478 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 7]. loss: 0.628044 acc: 0.775203 f1: 0.512434 time: 10.6 s\n",
      "**************************************************\n",
      "save model...\n",
      "learning rate: 0.000970\n",
      "[8 50] loss: 0.290995 metric: 0.846154 time: 1.5 s\n",
      "[8 100] loss: 0.309867 metric: 0.761905 time: 1.4 s\n",
      "[8 150] loss: 0.315619 metric: 0.795918 time: 1.5 s\n",
      "[8 200] loss: 0.331899 metric: 0.686567 time: 1.4 s\n",
      "[8 250] loss: 0.338594 metric: 0.753623 time: 1.4 s\n",
      "[8 300] loss: 0.334654 metric: 0.705882 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 8]. loss: 0.640588 acc: 0.752882 f1: 0.498018 time: 10.7 s\n",
      "**************************************************\n",
      "learning rate: 0.000970\n",
      "[9 50] loss: 0.257348 metric: 0.747664 time: 1.5 s\n",
      "[9 100] loss: 0.273201 metric: 0.754717 time: 1.5 s\n",
      "[9 150] loss: 0.286193 metric: 0.752294 time: 1.4 s\n",
      "[9 200] loss: 0.289440 metric: 0.758621 time: 1.4 s\n",
      "[9 250] loss: 0.291117 metric: 0.725926 time: 1.4 s\n",
      "[9 300] loss: 0.298061 metric: 0.769231 time: 1.3 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 9]. loss: 0.695931 acc: 0.759163 f1: 0.504330 time: 10.5 s\n",
      "**************************************************\n",
      "learning rate: 0.000961\n",
      "[10 50] loss: 0.222516 metric: 0.823529 time: 1.5 s\n",
      "[10 100] loss: 0.231905 metric: 0.787234 time: 1.4 s\n",
      "[10 150] loss: 0.253695 metric: 0.750000 time: 1.4 s\n",
      "[10 200] loss: 0.252557 metric: 0.677966 time: 1.4 s\n",
      "[10 250] loss: 0.253750 metric: 0.793388 time: 1.4 s\n",
      "[10 300] loss: 0.262653 metric: 0.762712 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 10]. loss: 0.867943 acc: 0.783680 f1: 0.504124 time: 10.5 s\n",
      "**************************************************\n",
      "learning rate: 0.000961\n",
      "[11 50] loss: 0.174137 metric: 0.886364 time: 1.5 s\n",
      "[11 100] loss: 0.189931 metric: 0.771084 time: 1.4 s\n",
      "[11 150] loss: 0.216102 metric: 0.854369 time: 1.4 s\n",
      "[11 200] loss: 0.222411 metric: 0.809917 time: 1.4 s\n",
      "[11 250] loss: 0.223619 metric: 0.796296 time: 1.5 s\n",
      "[11 300] loss: 0.225877 metric: 0.764706 time: 1.5 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 11]. loss: 0.936351 acc: 0.792096 f1: 0.502263 time: 10.7 s\n",
      "**************************************************\n",
      "learning rate: 0.000951\n",
      "[12 50] loss: 0.143916 metric: 0.901961 time: 1.5 s\n",
      "[12 100] loss: 0.164171 metric: 0.867925 time: 1.4 s\n",
      "[12 150] loss: 0.172483 metric: 0.813559 time: 1.4 s\n",
      "[12 200] loss: 0.192561 metric: 0.792793 time: 1.4 s\n",
      "[12 250] loss: 0.198469 metric: 0.744186 time: 1.5 s\n",
      "[12 300] loss: 0.189481 metric: 0.810811 time: 1.4 s\n",
      "Evaluating....\n",
      "**************************************************\n",
      "[epoch 12]. loss: 1.029653 acc: 0.790084 f1: 0.491280 time: 10.4 s\n",
      "**************************************************\n",
      "early stop at [12] epoch!\n"
     ]
    }
   ],
   "source": [
    "print_every = 50\n",
    "best_state = None\n",
    "max_metric = 0\n",
    "I = getIdentityMatrix(BATCH_SIZE, ATTEN_HOPS)\n",
    "# model.to(DEVICE)\n",
    "def predict(model, data_dl, loss_func, device):\n",
    "    model.eval()\n",
    "    res_list = []\n",
    "    label_list = []\n",
    "    loss = 0\n",
    "    for text1, text2, label in data_dl:\n",
    "#         text1 = pad_seq(text1, STR_MAXLEN)\n",
    "#         text2 = pad_seq(text2, STR_MAXLEN)\n",
    "        y_pred, _1, _2 = model(text1, text2)\n",
    "        loss += loss_func(y_pred, label).data.cpu()\n",
    "        y_pred = y_pred.data.max(1)[1].cpu().numpy()\n",
    "        res_list.extend(y_pred)\n",
    "        label_list.extend(label.data.cpu().numpy())\n",
    "    acc = accuracy_score(res_list, label_list)\n",
    "    Precision = precision_score(res_list, label_list)\n",
    "    Recall = recall_score(res_list, label_list)\n",
    "    F1 = f1_score(res_list, label_list)\n",
    "    \n",
    "    return loss, (acc, Precision, Recall, F1)\n",
    "        \n",
    "def evaluate(model, txt1, txt2, y):\n",
    "    pred, _1, _2 = model(txt1, txt2)\n",
    "    out_batch = pred.data.max(1)[1].cpu().numpy()\n",
    "    F1 = f1_score(out_batch, y)\n",
    "    return F1\n",
    "\n",
    "def training_termination(valid_result):\n",
    "    if len(valid_result) >= 4:\n",
    "        if valid_result[-1] < valid_result[-2] and \\\n",
    "            valid_result[-2] < valid_result[-3] and \\\n",
    "            valid_result[-3] < valid_result[-4]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "valid_iter.create_batches()\n",
    "valid_batch_num = len(list(valid_iter.batches))\n",
    "print_flush('start train...')\n",
    "train_iter.create_batches()\n",
    "batch_num = len(list(train_iter.batches))\n",
    "print_flush('batch number %d '%batch_num)\n",
    "valid_result = []\n",
    "for epoch in range(EPOCHES):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('learning rate: %.6f'% param_group['lr'])\n",
    "    epoch_begin = time()\n",
    "    total_loss = 0.0\n",
    "    train_iter.init_epoch()\n",
    "    batch_count = 0\n",
    "    batch_begin_time = time()\n",
    "    for text1, text2, label in train_dl:\n",
    "#         text1 = pad_seq(text1, STR_MAXLEN)\n",
    "#         text2 = pad_seq(text2, STR_MAXLEN)\n",
    "        model.train()\n",
    "        y_pred, attention1, attention2 = model(text1, text2)\n",
    "        attention1T = torch.transpose(attention1, 1, 2).contiguous()\n",
    "        attention2T = torch.transpose(attention2, 1, 2).contiguous()\n",
    "        extra_loss1 = Frobenius(torch.bmm(attention1, attention1T) - I[:attention1.size(0)])\n",
    "        extra_loss2 = Frobenius(torch.bmm(attention2, attention2T) - I[:attention2.size(0)])\n",
    "        loss = criterion(y_pred, label) + extra_loss1 + extra_loss2\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        if batch_count % print_every == 0:\n",
    "            metric = evaluate(model.eval(), text1, text2, label)\n",
    "            print_flush('[%d %d] loss: %.6f metric: %.6f time: %.1f s' %\n",
    "                  (epoch + 1, batch_count, total_loss / print_every, metric, time() - batch_begin_time))\n",
    "            total_loss = 0.0\n",
    "            batch_begin_time = time()\n",
    "    scheduler.step()\n",
    "    print_flush(\"Evaluating....\")\n",
    "    loss, (acc, Precision, Recall, F1) = predict(model, valid_dl, criterion, DEVICE)\n",
    "    valid_result.append(F1)\n",
    "    print_flush('*'*50)\n",
    "    print_flush('[epoch %d]. loss: %.6f acc: %.6f f1: %.6f time: %.1f s'%(epoch+1, loss/valid_batch_num, acc, F1, time()-epoch_begin))\n",
    "    print_flush('*'*50)\n",
    "    if F1 > max_metric:\n",
    "        best_state = model.state_dict()\n",
    "        max_metric = F1\n",
    "        print_flush(\"save model...\")\n",
    "#         torch.save(best_state, '../datasets/models/baseline_LSTM.pth')\n",
    "    epoch_begin = time()\n",
    "    if training_termination(valid_result):\n",
    "        print_flush(\"early stop at [%d] epoch!\" % (epoch+1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(5, 5, batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = torch.randn(1, 2, 5)\n",
    "out = lstm(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_pool = nn.AvgPool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = out[0].unsqueeze_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 10])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
