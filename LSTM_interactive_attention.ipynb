{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、普通attention 560\n",
    "\n",
    "2、普通ESIM 569\n",
    "\n",
    "3、加入full pretrain的ESIM 5779  加大deep到450 5796  HIDDEN_DIM = 200 DEEP_LAYERS = [500] 5811\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dic = torch.load(\"../datasets/models/KFold/ESIM_fold_30.5847.pth\")\n",
    "# dic2 = torch.load(\"../datasets/models/KFold/ESIM_fold_40.5779.pth\")\n",
    "# dic3 = torch.load(\"../datasets/models/KFold/ESIM_fold_70.5780.pth\")\n",
    "# for k,v in dic.items():\n",
    "#     dic[k] = v.cpu()\n",
    "# for k,v in dic2.items():\n",
    "#     dic2[k] = v.cpu()\n",
    "# for k,v in dic3.items():\n",
    "#     dic3[k] = v.cpu()\n",
    "# torch.save(dic, '../datasets/models/KFold/esim_fold_3_5847_cpu.pth')\n",
    "# torch.save(dic2, '../datasets/models/KFold/esim_fold_4_5779_cpu.pth')\n",
    "# torch.save(dic3, '../datasets/models/KFold/esim_fold_7_5780_cpu.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train = pd.read_csv('../datasets/atec_nlp_sim_train.csv',header=None,delimiter='\\t',names=['id','txt1','txt2','label'],encoding='utf-8')\n",
    "# train_add = pd.read_csv('../datasets/atec_nlp_sim_train_add.csv',header=None,delimiter='\\t',names=['id','txt1','txt2','label'], encoding='utf-8')\n",
    "# train = pd.concat([train, train_add])\n",
    "# split = StratifiedShuffleSplit(random_state=2018, test_size=0.16)\n",
    "\n",
    "# y = train.label.values\n",
    "# for tra_index, val_index in split.split(train, y):\n",
    "#     real_train = train.iloc[tra_index]\n",
    "#     real_valid = train.iloc[val_index]\n",
    "    \n",
    "# real_train, real_valid = train_test_split(train, test_size=0.19, random_state=2018)\n",
    "# real_train.to_csv('../datasets/train_random.csv', index=False, encoding='utf-8')\n",
    "# real_valid.to_csv('../datasets/valid_random.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchtext import data\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# hyper parameters\n",
    "STR_MAXLEN = 30\n",
    "BATCH_SIZE = 256\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 200\n",
    "DEEP_LAYERS = [500]\n",
    "LEARNING_RATE = 1e-3*0.95\n",
    "EPOCHES = 20\n",
    "DECAY_STEP = 2\n",
    "DECAY_GAMMA = 0.99\n",
    "CLASS_WEIGHT = [0.6116, 2.7397]\n",
    "def print_flush(data, args=None):\n",
    "    if args == None:\n",
    "        print(data)\n",
    "    else:\n",
    "        print(data, args)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def pad_seq(seq, max_length):\n",
    "    length = len(seq[0])\n",
    "    pad_leng = 0 if length > max_length else (max_length-length)\n",
    "    if pad_leng == 0:\n",
    "        seq = seq[:, :max_length]\n",
    "    else:\n",
    "        seq = torch.cat([seq, torch.ones(len(seq), pad_leng).long()], dim=1)\n",
    "    return seq\n",
    "\n",
    "def get_mask(txt1, txt2):\n",
    "    txt1 = txt1.cpu().numpy()\n",
    "    txt2 = txt2.cpu().numpy()\n",
    "    len_txt1 = len(txt1[0])\n",
    "    len_txt2 = len(txt2[0])\n",
    "    mask_txt1 = np.zeros(shape=[len(txt1), len_txt1])\n",
    "    mask_txt2 = np.zeros(shape=[len(txt2), len_txt2])\n",
    "    for i in range(len(txt1)):\n",
    "        mask_len1 = np.where(txt1[i] == 1)[0]\n",
    "        mask_len2 = np.where(txt2[i] == 1)[0]\n",
    "        if len(mask_len1) == 0:\n",
    "            mask_txt1[i, :] = 1\n",
    "        else:\n",
    "            mask_txt1[i, :mask_len1[0]] = 1\n",
    "        if len(mask_len2) == 0:\n",
    "            mask_txt2[i, :] = 1\n",
    "        else:\n",
    "            mask_txt2[i, :mask_len2[0]] = 1\n",
    "    return torch.tensor(mask_txt1).float().to(DEVICE), torch.tensor(mask_txt2).float().to(DEVICE)\n",
    "\n",
    "def wordlist_to_matrix(pretrain_path, wordlist, device, dim=300):\n",
    "    word_vec = {}\n",
    "    with open(pretrain_path, encoding='utf-8') as fr:\n",
    "        for line in fr:\n",
    "            line = line.split(' ')\n",
    "            word = line[0]\n",
    "            vec = line[1:]\n",
    "            word_vec[word] = np.array(vec, dtype=float)\n",
    "    word_vec_list = []\n",
    "    oov = 0\n",
    "    oov_words = []\n",
    "    for idx, word in enumerate(wordlist):\n",
    "        try:\n",
    "            vector = np.array(word_vec[word], dtype=float).reshape(1,dim)\n",
    "        except:\n",
    "            oov += 1\n",
    "            oov_words.append(word)\n",
    "            # print(word)\n",
    "            vector = np.random.rand(1, dim)\n",
    "        word_vec_list.append(torch.from_numpy(vector))\n",
    "    wordvec_matrix = torch.cat(word_vec_list)\n",
    "    print(\"Load embedding finished.\")\n",
    "    print(\"Total words count: {}, oov count: {}.\".format(wordvec_matrix.size()[0], oov))\n",
    "    return wordvec_matrix if device == -1 else wordvec_matrix.to(device)\n",
    "\n",
    "# torch.manual_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process raw data...\n",
      "Building vocabulary Finished.\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [txt for txt in text]\n",
    "class BatchWrapper:\n",
    "    def __init__(self, dl, iter_columns):\n",
    "        self.dl, self.iter_columns = dl, iter_columns  # we pass in the list of attributes for x &amp;amp;amp;amp;lt;g class=\"gr_ gr_3178 gr-alert gr_spell gr_inline_cards gr_disable_anim_appear ContextualSpelling ins-del\" id=\"3178\" data-gr-id=\"3178\"&amp;amp;amp;amp;gt;and y&amp;amp;amp;amp;lt;/g&amp;amp;amp;amp;gt;\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield (getattr(batch, attr) for attr in self.iter_columns)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "print_flush('process raw data...')\n",
    "TEXT = data.Field(sequential=True, use_vocab=True, eos_token='<EOS>', init_token='<BOS>',pad_token='<PAD>', \n",
    "                  batch_first=True, tokenize=tokenizer)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "\n",
    "tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"txt1\", TEXT), (\"txt2\", TEXT),\n",
    "                 (\"label\", LABEL)]\n",
    "\n",
    "train = data.TabularDataset(path='../datasets/train_random.csv', format='csv', skip_header=True, fields=tv_datafields)\n",
    "valid = data.TabularDataset(path='../datasets/valid_random.csv', format='csv', skip_header=True, fields=tv_datafields)\n",
    "\n",
    "TEXT.build_vocab(train, valid, min_freq=3)\n",
    "print_flush('Building vocabulary Finished.')\n",
    "\n",
    "matrix = wordlist_to_matrix('../datasets/pretrain_embedding/pretrain_full_emb.txt', TEXT.vocab.itos, DEVICE)\n",
    "# matrix = wordmodel_to_matrix('../datasets/pretrain_embedding/raw_embedding_300d.bin', TEXT.vocab.itos, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1517"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare data done!\n"
     ]
    }
   ],
   "source": [
    "train_iter = data.BucketIterator(dataset=train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.txt1) + len(x.txt2), shuffle=False, device=-1, repeat=False)\n",
    "valid_iter = data.Iterator(dataset=valid, batch_size=BATCH_SIZE, device=-1, shuffle=False, repeat=False)\n",
    "\n",
    "train_dl = BatchWrapper(train_iter, [\"txt1\", \"txt2\", \"label\"])\n",
    "valid_dl = BatchWrapper(valid_iter, [\"txt1\", \"txt2\", \"label\"])\n",
    "\n",
    "print_flush('prepare data done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x1, x2):\n",
    "    '''compute cosine similarity between x1 and x2\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1, x2 : 2-D torch Tensor\n",
    "        size (batch_size, 1)\n",
    "    Returns\n",
    "    -------\n",
    "    distance : 2-D torch Tensor\n",
    "        similarity result of size (batch_size, 1)\n",
    "    '''\n",
    "    return F.cosine_similarity(x1, x2).unsqueeze(1)\n",
    "def attention_matrix(x1, x2, eps=1e-6):\n",
    "    '''compute attention matrix using match score\n",
    "    \n",
    "    1 / (1 + |x · y|)\n",
    "    |·| is euclidean distance\n",
    "    Parameters\n",
    "    ----------\n",
    "    x1, x2 : 4-D torch Tensor\n",
    "        size (batch_size, 1, sentence_length, h)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : 3-D torch Tensor\n",
    "        match score result of size (batch_size, sentence_length(for x2), sentence_length(for x1))\n",
    "    '''\n",
    "    eps = torch.tensor(eps).to(DEVICE)\n",
    "    one = torch.tensor(1.).to(DEVICE)\n",
    "    euclidean = (torch.pow(x1 - x2.permute(0, 2, 1, 3), 2).sum(dim=3) + eps).sqrt()\n",
    "    return (euclidean + one).reciprocal()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from time import time\n",
    "\n",
    "class LSTM_INTERACTIVE_ATTENTION(nn.Module):\n",
    "    def __init__(self, vocab, embed_dim, hidden_dim, deep_layers, pretrain_embed=torch.tensor([]), is_batch_norm=False, is_drop_out=False):\n",
    "        super(LSTM_INTERACTIVE_ATTENTION, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.deep_layers = deep_layers\n",
    "        self.is_batch_norm = is_batch_norm\n",
    "        self.is_drop_out = is_drop_out\n",
    "        self.word_embed = nn.Embedding(len(vocab), embed_dim, padding_idx=vocab.stoi['<PAD>'])\n",
    "        if len(pretrain_embed) != 0:\n",
    "            self.word_embed.weight.data.copy_(pretrain_embed)\n",
    "#             self.word_embed.weight.requires_grad = False\n",
    "\n",
    "        self.lstm_embed = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "#         self.attention = Attention(2*hidden_dim, hidden_dim)\n",
    "        self.lstm_fusion = nn.LSTM(2 * hidden_dim * 4, hidden_dim*2, batch_first=True, bidirectional=True)\n",
    "        self.linear_1 = nn.Linear(2*hidden_dim*4, deep_layers[0])\n",
    "#         glorot = np.sqrt(6.0 / (hidden_dim*4 + self.deep_layers[0]))\n",
    "#         self.linear_1.weight.data.uniform_(-glorot, glorot)\n",
    "        if self.is_batch_norm:\n",
    "            self.batch_norm_1 = nn.BatchNorm1d(deep_layers[0])\n",
    "        if self.is_drop_out:\n",
    "            self.linear_1_dropout = nn.Dropout(0.1)\n",
    "        for i, h in enumerate(self.deep_layers[1:], 1):\n",
    "            setattr(self,'linear_'+str(i+1), nn.Linear(self.deep_layers[i-1], self.deep_layers[i]))\n",
    "            if self.is_batch_norm:\n",
    "                setattr(self, 'batch_norm_' + str(i + 1), nn.BatchNorm1d(deep_layers[i]))\n",
    "            if self.is_drop_out:\n",
    "                setattr(self, 'linear_'+str(i+1)+'_dropout', nn.Dropout(0.8))\n",
    "        self.deep_out = nn.Linear(deep_layers[-1], 2)\n",
    "#         self.attention_feature_layer = nn.Linear(STR_MAXLEN, hidden_dim*2)\n",
    "#         self.deep_dropout_1 = nn.Dropout(0.5)\n",
    "    def forward(self, txt1, txt2, x1_mask, x2_mask, hidden=None):\n",
    "        embed_txt1 = self.word_embed(txt1)\n",
    "        embed_txt2 = self.word_embed(txt2)\n",
    "#         lstm_txt1 = torch.sum(self.lstm_embed(embed_txt1, hidden)[0], 1)\n",
    "#         lstm_txt2 = torch.sum(self.lstm_embed(embed_txt2, hidden)[0], 1)\n",
    "        lstm_txt1 = self.lstm_embed(embed_txt1, None)[0]* x1_mask[:, :, None]\n",
    "        lstm_txt2 = self.lstm_embed(embed_txt2, None)[0]* x2_mask[:, :, None]\n",
    "#         lstm_txt1 = lstm_txt1 \n",
    "#         lstm_txt2 = lstm_txt2 \n",
    "        \n",
    "        weight_matrix = torch.bmm(lstm_txt1, lstm_txt2.permute(0, 2, 1))\n",
    "        weight_matrix_1 = torch.exp(weight_matrix - weight_matrix.max(2, keepdim=True)[0])\n",
    "        weight_matrix_2 = torch.exp(weight_matrix - weight_matrix.max(1, keepdim=True)[0])\n",
    "    \n",
    "        alphas = weight_matrix_1 / weight_matrix_1.sum(2, keepdim=True)\n",
    "        beta = weight_matrix_2 / weight_matrix_2.sum(1, keepdim=True)\n",
    "        alphas = alphas * x1_mask[:, :, None]\n",
    "        beta = beta * x2_mask[:, None, :]\n",
    "        \n",
    "        atten_txt1to2 = torch.bmm(alphas, lstm_txt2)\n",
    "        atten_txt2to1 = torch.bmm(beta.permute(0, 2, 1), lstm_txt1)\n",
    "        \n",
    "#         atten_txt1to2, atten_txt2to1 = self.attention(lstm_txt1, lstm_txt2)\n",
    "        \n",
    "        txt_substract1 = torch.abs(lstm_txt1-atten_txt1to2)\n",
    "        txt_substract2 = torch.abs(lstm_txt2-atten_txt2to1)\n",
    "        \n",
    "        txt_multiply1 = torch.mul(lstm_txt1, atten_txt1to2)\n",
    "        txt_multiply2 = torch.mul(lstm_txt2, atten_txt2to1)\n",
    "        \n",
    "        out_txt1 = torch.cat([lstm_txt1, atten_txt1to2, txt_substract1, txt_multiply1], 2)\n",
    "        out_txt2 = torch.cat([lstm_txt2, atten_txt2to1, txt_substract2, txt_multiply2], 2)\n",
    "        lstm_fusion_txt1 = self.lstm_fusion(out_txt1, None)[0]\n",
    "        lstm_fusion_txt2 = self.lstm_fusion(out_txt2, None)[0]\n",
    "        lstm_fusion_txt1 = lstm_fusion_txt1.unsqueeze(1)\n",
    "        lstm_fusion_txt2 = lstm_fusion_txt2.unsqueeze(1)\n",
    "        lstm_txt1_max = F.max_pool2d(lstm_fusion_txt1, (len(txt1[0]), 1)).squeeze()\n",
    "        lstm_txt2_max = F.max_pool2d(lstm_fusion_txt2, (len(txt2[0]), 1)).squeeze() \n",
    "#         lstm_txt1_max = F.max_pool2d(lstm_txt1, (len(txt1[0]), 1)).squeeze()\n",
    "#         lstm_txt2_max = F.max_pool2d(lstm_txt2, (len(txt2[0]), 1)).squeeze() \n",
    "        \n",
    "        \n",
    "#         lstm_txt1 = lstm_txt1.squeeze()\n",
    "#         lstm_txt2 = lstm_txt2.squeeze()\n",
    "        \n",
    "        output = self.linear_1(torch.cat([lstm_txt1_max, lstm_txt2_max], 1))\n",
    "        if self.is_batch_norm:\n",
    "            output = self.batch_norm_1(output)\n",
    "        output = F.relu(output)\n",
    "        if self.is_drop_out:\n",
    "            output = self.linear_1_dropout(output)\n",
    "        for i in range(1, len(self.deep_layers)):\n",
    "            output = getattr(self, 'linear_' + str(i + 1))(output)\n",
    "            if self.is_batch_norm:\n",
    "                output = getattr(self, 'batch_norm_' + str(i + 1))(output)\n",
    "            output = F.relu(output)\n",
    "            if self.is_drop_out:\n",
    "                output = getattr(self, 'linear_' + str(i + 1) + '_dropout')(output)\n",
    "        output = self.deep_out(output)\n",
    "        return F.log_softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LSTM_INTERACTIVE_ATTENTION(TEXT.vocab, EMBED_DIM, HIDDEN_DIM, DEEP_LAYERS, matrix)\n",
    "model.to(DEVICE)\n",
    "parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "criterion = nn.NLLLoss(weight=torch.tensor(CLASS_WEIGHT).float().to(DEVICE))\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "# scheduler = StepLR(optimizer, step_size=DECAY_STEP, gamma=DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_every = 50\n",
    "best_state = None\n",
    "max_metric = 0\n",
    "# model.to(DEVICE)\n",
    "def predict(model, data_dl, loss_func, device):\n",
    "    model.eval()\n",
    "    res_list = []\n",
    "    label_list = []\n",
    "    loss = 0\n",
    "    for text1, text2, label in data_dl:\n",
    "        text1 = text1.to(DEVICE)\n",
    "        text2 = text2.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        x1_mask, x2_mask = get_mask(text1, text2)\n",
    "        y_pred = model(text1, text2, x1_mask, x2_mask)\n",
    "        loss += loss_func(y_pred, label).data.cpu()\n",
    "        y_pred = y_pred.data.max(1)[1].cpu().numpy()\n",
    "        res_list.extend(y_pred)\n",
    "        label_list.extend(label.data.cpu().numpy())\n",
    "    acc = accuracy_score(res_list, label_list)\n",
    "    Precision = precision_score(res_list, label_list)\n",
    "    Recall = recall_score(res_list, label_list)\n",
    "    F1 = f1_score(res_list, label_list)\n",
    "    \n",
    "    return loss, (acc, Precision, Recall, F1)\n",
    "        \n",
    "def evaluate(model, txt1, txt2, x1_mask, x2_mask, y):\n",
    "    pred = model(txt1, txt2, x1_mask, x2_mask)\n",
    "    out_batch = pred.data.max(1)[1].cpu().numpy()\n",
    "    F1 = f1_score(out_batch, y)\n",
    "    return F1\n",
    "\n",
    "def training_termination(valid_result):\n",
    "    if len(valid_result) >= 4:\n",
    "        if valid_result[-1] < valid_result[-2] and \\\n",
    "            valid_result[-2] < valid_result[-3] and \\\n",
    "            valid_result[-3] < valid_result[-4]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "valid_iter.create_batches()\n",
    "valid_batch_num = len(list(valid_iter.batches))\n",
    "print_flush('start train...')\n",
    "train_iter.create_batches()\n",
    "batch_num = len(list(train_iter.batches))\n",
    "print_flush('batch number %d '%batch_num)\n",
    "valid_result = []\n",
    "for epoch in range(EPOCHES):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('learning rate: %.6f'% param_group['lr'])\n",
    "    epoch_begin = time()\n",
    "    total_loss = 0.0\n",
    "    train_iter.init_epoch()\n",
    "    batch_count = 0\n",
    "    batch_begin_time = time()\n",
    "    for text1, text2, label in train_dl:\n",
    "        model.train()\n",
    "        text1 = text1.to(DEVICE)\n",
    "        text2 = text2.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        x1_mask, x2_mask = get_mask(text1, text2)\n",
    "        y_pred = model(text1, text2, x1_mask, x2_mask)\n",
    "        loss = criterion(y_pred, label)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        if batch_count % print_every == 0:\n",
    "            metric = evaluate(model.eval(), text1, text2, x1_mask, x2_mask, label)\n",
    "            print_flush('[%d %d] loss: %.6f metric: %.6f time: %.1f s' %\n",
    "                  (epoch + 1, batch_count, total_loss / print_every, metric, time() - batch_begin_time))\n",
    "            total_loss = 0.0\n",
    "            batch_begin_time = time()\n",
    "#     scheduler.step()\n",
    "    print_flush(\"Evaluating....\")\n",
    "    loss, (acc, Precision, Recall, F1) = predict(model, valid_dl, criterion, DEVICE)\n",
    "    valid_result.append(F1)\n",
    "    print_flush('*'*60)\n",
    "    print_flush('[epoch %d]. loss: %.6f acc: %.6f f1: %.6f time: %.1f s'%(epoch+1, loss/valid_batch_num, acc, F1, time()-epoch_begin))\n",
    "    print_flush('*'*60)\n",
    "    if F1 > max_metric:\n",
    "        best_state = model.state_dict()\n",
    "        max_metric = F1\n",
    "        print_flush(\"save model...\")\n",
    "#         torch.save(best_state, '../datasets/models/baseline_LSTM.pth')\n",
    "    epoch_begin = time()\n",
    "    if training_termination(valid_result):\n",
    "        print_flush(\"early stop at [%d] epoch!\" % (epoch+1))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold  1\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:139: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 50] loss: 0.688105 metric: 0.379747 time: 9.8 s\n",
      "[1 100] loss: 0.674363 metric: 0.476562 time: 9.7 s\n",
      "[1 150] loss: 0.628048 metric: 0.196078 time: 10.2 s\n",
      "[1 200] loss: 0.612377 metric: 0.450867 time: 9.9 s\n",
      "[1 250] loss: 0.594764 metric: 0.428571 time: 10.1 s\n",
      "[1 300] loss: 0.564387 metric: 0.500000 time: 10.1 s\n",
      "[1 350] loss: 0.567959 metric: 0.477876 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.539375 acc: 0.756928 f1: 0.495851 time: 74.4 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.517073 metric: 0.641509 time: 9.9 s\n",
      "[2 100] loss: 0.483164 metric: 0.604651 time: 9.8 s\n",
      "[2 150] loss: 0.472346 metric: 0.542056 time: 10.3 s\n",
      "[2 200] loss: 0.497346 metric: 0.636364 time: 9.9 s\n",
      "[2 250] loss: 0.489188 metric: 0.564706 time: 10.2 s\n",
      "[2 300] loss: 0.478531 metric: 0.600000 time: 10.2 s\n",
      "[2 350] loss: 0.494633 metric: 0.580000 time: 10.3 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.524457 acc: 0.816062 f1: 0.550870 time: 75.1 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.436756 metric: 0.662577 time: 9.9 s\n",
      "[3 100] loss: 0.408402 metric: 0.650602 time: 9.9 s\n",
      "[3 150] loss: 0.412418 metric: 0.603774 time: 10.4 s\n",
      "[3 200] loss: 0.445044 metric: 0.666667 time: 9.9 s\n",
      "[3 250] loss: 0.438439 metric: 0.577778 time: 10.1 s\n",
      "[3 300] loss: 0.432393 metric: 0.653061 time: 10.2 s\n",
      "[3 350] loss: 0.452049 metric: 0.660194 time: 10.3 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.538603 acc: 0.820160 f1: 0.562545 time: 75.1 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.389314 metric: 0.692308 time: 9.9 s\n",
      "[4 100] loss: 0.363988 metric: 0.700000 time: 9.8 s\n",
      "[4 150] loss: 0.368702 metric: 0.616667 time: 10.4 s\n",
      "[4 200] loss: 0.404736 metric: 0.671875 time: 10.0 s\n",
      "[4 250] loss: 0.388268 metric: 0.645161 time: 10.1 s\n",
      "[4 300] loss: 0.391852 metric: 0.673684 time: 10.1 s\n",
      "[4 350] loss: 0.407948 metric: 0.672727 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.580506 acc: 0.812646 f1: 0.556787 time: 75.2 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.344184 metric: 0.745098 time: 9.9 s\n",
      "[5 100] loss: 0.319188 metric: 0.748466 time: 9.9 s\n",
      "[5 150] loss: 0.328495 metric: 0.603175 time: 10.4 s\n",
      "[5 200] loss: 0.359332 metric: 0.718750 time: 9.9 s\n",
      "[5 250] loss: 0.334760 metric: 0.735632 time: 10.1 s\n",
      "[5 300] loss: 0.348444 metric: 0.729167 time: 10.1 s\n",
      "[5 350] loss: 0.367779 metric: 0.711538 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.694496 acc: 0.823575 f1: 0.556645 time: 75.1 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.303251 metric: 0.770270 time: 9.8 s\n",
      "[6 100] loss: 0.271973 metric: 0.805195 time: 9.8 s\n",
      "[6 150] loss: 0.293436 metric: 0.616541 time: 10.4 s\n",
      "[6 200] loss: 0.335917 metric: 0.691729 time: 10.0 s\n",
      "[6 250] loss: 0.282565 metric: 0.776471 time: 10.1 s\n",
      "[6 300] loss: 0.339914 metric: 0.614173 time: 10.2 s\n",
      "[6 350] loss: 0.342463 metric: 0.723810 time: 10.3 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.641342 acc: 0.794009 f1: 0.549028 time: 74.9 s\n",
      "************************************************************\n",
      "early stop at [6] epoch!\n",
      "fold  2\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.688728 metric: 0.364217 time: 9.8 s\n",
      "[1 100] loss: 0.681646 metric: 0.492462 time: 9.7 s\n",
      "[1 150] loss: 0.632084 metric: 0.421053 time: 10.3 s\n",
      "[1 200] loss: 0.615732 metric: 0.496454 time: 10.0 s\n",
      "[1 250] loss: 0.597607 metric: 0.543689 time: 10.2 s\n",
      "[1 300] loss: 0.568317 metric: 0.446281 time: 10.2 s\n",
      "[1 350] loss: 0.563736 metric: 0.475410 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.522466 acc: 0.730679 f1: 0.503240 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.510099 metric: 0.650000 time: 9.7 s\n",
      "[2 100] loss: 0.486135 metric: 0.622222 time: 9.7 s\n",
      "[2 150] loss: 0.464150 metric: 0.550459 time: 10.3 s\n",
      "[2 200] loss: 0.493696 metric: 0.624000 time: 10.0 s\n",
      "[2 250] loss: 0.493357 metric: 0.620000 time: 10.2 s\n",
      "[2 300] loss: 0.479046 metric: 0.533333 time: 10.2 s\n",
      "[2 350] loss: 0.492314 metric: 0.540541 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.492598 acc: 0.803962 f1: 0.560874 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.434622 metric: 0.658065 time: 9.7 s\n",
      "[3 100] loss: 0.413839 metric: 0.674157 time: 9.7 s\n",
      "[3 150] loss: 0.400781 metric: 0.557377 time: 10.3 s\n",
      "[3 200] loss: 0.445043 metric: 0.677165 time: 10.0 s\n",
      "[3 250] loss: 0.444067 metric: 0.618557 time: 10.2 s\n",
      "[3 300] loss: 0.430654 metric: 0.578512 time: 10.2 s\n",
      "[3 350] loss: 0.451539 metric: 0.571429 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.512628 acc: 0.820062 f1: 0.571760 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.387018 metric: 0.701299 time: 9.7 s\n",
      "[4 100] loss: 0.367973 metric: 0.715909 time: 9.6 s\n",
      "[4 150] loss: 0.352241 metric: 0.595041 time: 10.3 s\n",
      "[4 200] loss: 0.404225 metric: 0.682927 time: 10.0 s\n",
      "[4 250] loss: 0.400761 metric: 0.659574 time: 10.2 s\n",
      "[4 300] loss: 0.386358 metric: 0.601626 time: 10.2 s\n",
      "[4 350] loss: 0.403889 metric: 0.619469 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.535088 acc: 0.814403 f1: 0.572200 time: 74.6 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.345032 metric: 0.723684 time: 9.7 s\n",
      "[5 100] loss: 0.318283 metric: 0.750000 time: 9.7 s\n",
      "[5 150] loss: 0.301715 metric: 0.634146 time: 10.3 s\n",
      "[5 200] loss: 0.352897 metric: 0.715447 time: 10.0 s\n",
      "[5 250] loss: 0.352049 metric: 0.742268 time: 10.3 s\n",
      "[5 300] loss: 0.342901 metric: 0.625000 time: 10.2 s\n",
      "[5 350] loss: 0.359797 metric: 0.637168 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.637714 acc: 0.821526 f1: 0.568327 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.300301 metric: 0.780822 time: 9.7 s\n",
      "[6 100] loss: 0.272473 metric: 0.774566 time: 9.7 s\n",
      "[6 150] loss: 0.267595 metric: 0.655738 time: 10.2 s\n",
      "[6 200] loss: 0.349437 metric: 0.666667 time: 10.0 s\n",
      "[6 250] loss: 0.316356 metric: 0.817204 time: 10.2 s\n",
      "[6 300] loss: 0.297999 metric: 0.714286 time: 10.2 s\n",
      "[6 350] loss: 0.374289 metric: 0.636364 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.568221 acc: 0.788837 f1: 0.564588 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[7 50] loss: 0.262010 metric: 0.788732 time: 9.7 s\n",
      "[7 100] loss: 0.266321 metric: 0.739130 time: 9.7 s\n",
      "[7 150] loss: 0.244341 metric: 0.829268 time: 10.3 s\n",
      "[7 200] loss: 0.323398 metric: 0.643836 time: 10.0 s\n",
      "[7 250] loss: 0.285358 metric: 0.860215 time: 10.2 s\n",
      "[7 300] loss: 0.302054 metric: 0.738739 time: 10.2 s\n",
      "[7 350] loss: 0.307873 metric: 0.710280 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 7]. loss: 0.684878 acc: 0.797131 f1: 0.566604 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[8 50] loss: 0.226967 metric: 0.814286 time: 9.7 s\n",
      "[8 100] loss: 0.256735 metric: 0.827160 time: 9.7 s\n",
      "[8 150] loss: 0.212961 metric: 0.683761 time: 10.3 s\n",
      "[8 200] loss: 0.283446 metric: 0.771930 time: 10.0 s\n",
      "[8 250] loss: 0.252056 metric: 0.840000 time: 10.2 s\n",
      "[8 300] loss: 0.289814 metric: 0.740741 time: 10.2 s\n",
      "[8 350] loss: 0.267136 metric: 0.629921 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 8]. loss: 0.774508 acc: 0.817428 f1: 0.574483 time: 74.7 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[9 50] loss: 0.215808 metric: 0.814286 time: 9.7 s\n",
      "[9 100] loss: 0.233733 metric: 0.829268 time: 9.7 s\n",
      "[9 150] loss: 0.173119 metric: 0.761905 time: 10.2 s\n",
      "[9 200] loss: 0.237891 metric: 0.836364 time: 10.0 s\n",
      "[9 250] loss: 0.243802 metric: 0.604317 time: 10.2 s\n",
      "[9 300] loss: 0.248716 metric: 0.773585 time: 10.2 s\n",
      "[9 350] loss: 0.255231 metric: 0.661017 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 9]. loss: 1.027792 acc: 0.842603 f1: 0.542671 time: 74.5 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[10 50] loss: 0.204818 metric: 0.754967 time: 9.7 s\n",
      "[10 100] loss: 0.195829 metric: 0.829268 time: 9.6 s\n",
      "[10 150] loss: 0.145450 metric: 0.860215 time: 10.3 s\n",
      "[10 200] loss: 0.230875 metric: 0.690647 time: 10.0 s\n",
      "[10 250] loss: 0.203938 metric: 0.863158 time: 10.2 s\n",
      "[10 300] loss: 0.206016 metric: 0.752294 time: 10.2 s\n",
      "[10 350] loss: 0.220886 metric: 0.808989 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 10]. loss: 0.809705 acc: 0.819379 f1: 0.562928 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[11 50] loss: 0.187278 metric: 0.835821 time: 9.7 s\n",
      "[11 100] loss: 0.201716 metric: 0.906667 time: 9.7 s\n",
      "[11 150] loss: 0.156056 metric: 0.930233 time: 10.2 s\n",
      "[11 200] loss: 0.178402 metric: 0.768000 time: 10.0 s\n",
      "[11 250] loss: 0.174078 metric: 0.913043 time: 10.2 s\n",
      "[11 300] loss: 0.187534 metric: 0.803922 time: 10.1 s\n",
      "[11 350] loss: 0.173555 metric: 0.869565 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 11]. loss: 0.866469 acc: 0.790105 f1: 0.548583 time: 74.5 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[12 50] loss: 0.163248 metric: 0.919355 time: 9.7 s\n",
      "[12 100] loss: 0.171319 metric: 0.928571 time: 9.7 s\n",
      "[12 150] loss: 0.182821 metric: 0.860215 time: 10.2 s\n",
      "[12 200] loss: 0.144855 metric: 0.834783 time: 10.0 s\n",
      "[12 250] loss: 0.144325 metric: 0.933333 time: 10.2 s\n",
      "[12 300] loss: 0.190052 metric: 0.854167 time: 10.2 s\n",
      "[12 350] loss: 0.151643 metric: 0.898876 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 12]. loss: 0.866888 acc: 0.767564 f1: 0.541040 time: 74.6 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[13 50] loss: 0.153080 metric: 0.883721 time: 9.7 s\n",
      "[13 100] loss: 0.150297 metric: 0.931507 time: 9.7 s\n",
      "[13 150] loss: 0.189161 metric: 0.909091 time: 10.2 s\n",
      "[13 200] loss: 0.126252 metric: 0.864865 time: 10.0 s\n",
      "[13 250] loss: 0.141230 metric: 0.954545 time: 10.2 s\n",
      "[13 300] loss: 0.190419 metric: 0.796117 time: 10.2 s\n",
      "[13 350] loss: 0.133052 metric: 0.860215 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 13]. loss: 1.181443 acc: 0.817623 f1: 0.553405 time: 74.6 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[14 50] loss: 0.154753 metric: 0.949153 time: 9.5 s\n",
      "[14 100] loss: 0.146868 metric: 0.877419 time: 9.7 s\n",
      "[14 150] loss: 0.139482 metric: 0.941176 time: 10.2 s\n",
      "[14 200] loss: 0.108587 metric: 0.905660 time: 10.0 s\n",
      "[14 250] loss: 0.104401 metric: 0.965517 time: 10.2 s\n",
      "[14 300] loss: 0.176202 metric: 0.836735 time: 10.2 s\n",
      "[14 350] loss: 0.105556 metric: 0.909091 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 14]. loss: 2.082941 acc: 0.843482 f1: 0.502790 time: 74.5 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[15 50] loss: 0.164478 metric: 0.973913 time: 9.7 s\n",
      "[15 100] loss: 0.131960 metric: 0.855346 time: 9.7 s\n",
      "[15 150] loss: 0.108130 metric: 0.952381 time: 10.2 s\n",
      "[15 200] loss: 0.100194 metric: 0.864865 time: 9.9 s\n",
      "[15 250] loss: 0.078461 metric: 0.965517 time: 10.2 s\n",
      "[15 300] loss: 0.159754 metric: 0.854167 time: 10.2 s\n",
      "[15 350] loss: 0.091344 metric: 0.930233 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 15]. loss: 2.080970 acc: 0.843677 f1: 0.517760 time: 74.6 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[16 50] loss: 0.111033 metric: 0.957983 time: 9.8 s\n",
      "[16 100] loss: 0.109270 metric: 0.931507 time: 9.7 s\n",
      "[16 150] loss: 0.081409 metric: 0.930233 time: 10.2 s\n",
      "[16 200] loss: 0.098761 metric: 0.923077 time: 10.0 s\n",
      "[16 250] loss: 0.071931 metric: 0.943820 time: 10.1 s\n",
      "[16 300] loss: 0.134068 metric: 0.901099 time: 10.2 s\n",
      "[16 350] loss: 0.090193 metric: 0.909091 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 16]. loss: 2.291770 acc: 0.843677 f1: 0.504025 time: 74.6 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[17 50] loss: 0.098221 metric: 0.912000 time: 9.7 s\n",
      "[17 100] loss: 0.096325 metric: 0.971429 time: 9.6 s\n",
      "[17 150] loss: 0.073303 metric: 0.919540 time: 10.3 s\n",
      "[17 200] loss: 0.083122 metric: 0.960000 time: 10.0 s\n",
      "[17 250] loss: 0.068165 metric: 0.923077 time: 10.2 s\n",
      "[17 300] loss: 0.121636 metric: 0.953488 time: 10.2 s\n",
      "[17 350] loss: 0.086735 metric: 0.898876 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 17]. loss: 2.378783 acc: 0.843286 f1: 0.490159 time: 74.7 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[18 50] loss: 0.075747 metric: 0.957983 time: 9.7 s\n",
      "[18 100] loss: 0.060777 metric: 0.971429 time: 9.7 s\n",
      "[18 150] loss: 0.061092 metric: 0.941176 time: 10.2 s\n",
      "[18 200] loss: 0.068648 metric: 0.969697 time: 10.0 s\n",
      "[18 250] loss: 0.062992 metric: 0.943820 time: 10.3 s\n",
      "[18 300] loss: 0.114094 metric: 0.964706 time: 10.2 s\n",
      "[18 350] loss: 0.106306 metric: 0.963855 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 18]. loss: 1.681737 acc: 0.837139 f1: 0.537545 time: 74.9 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[19 50] loss: 0.079859 metric: 0.991304 time: 9.7 s\n",
      "[19 100] loss: 0.060882 metric: 0.957746 time: 9.7 s\n",
      "[19 150] loss: 0.055954 metric: 0.975610 time: 10.2 s\n",
      "[19 200] loss: 0.064894 metric: 0.941176 time: 10.0 s\n",
      "[19 250] loss: 0.044354 metric: 0.964706 time: 10.2 s\n",
      "[19 300] loss: 0.068715 metric: 0.942529 time: 10.2 s\n",
      "[19 350] loss: 0.112223 metric: 0.975610 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 19]. loss: 1.565851 acc: 0.823868 f1: 0.555309 time: 74.6 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[20 50] loss: 0.072400 metric: 0.982759 time: 9.7 s\n",
      "[20 100] loss: 0.057941 metric: 0.978417 time: 9.7 s\n",
      "[20 150] loss: 0.051244 metric: 0.987654 time: 10.3 s\n",
      "[20 200] loss: 0.053847 metric: 0.905660 time: 10.0 s\n",
      "[20 250] loss: 0.054684 metric: 0.976744 time: 10.2 s\n",
      "[20 300] loss: 0.047056 metric: 0.953488 time: 10.2 s\n",
      "[20 350] loss: 0.111467 metric: 0.941176 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 20]. loss: 1.480263 acc: 0.828454 f1: 0.559840 time: 74.9 s\n",
      "************************************************************\n",
      "fold  3\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.686616 metric: 0.387302 time: 9.7 s\n",
      "[1 100] loss: 0.679737 metric: 0.377778 time: 9.7 s\n",
      "[1 150] loss: 0.651729 metric: 0.410256 time: 10.3 s\n",
      "[1 200] loss: 0.622778 metric: 0.445596 time: 10.0 s\n",
      "[1 250] loss: 0.594242 metric: 0.423529 time: 10.1 s\n",
      "[1 300] loss: 0.569747 metric: 0.459259 time: 10.2 s\n",
      "[1 350] loss: 0.562821 metric: 0.466667 time: 10.2 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.522769 acc: 0.735265 f1: 0.514408 time: 74.9 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.503221 metric: 0.655367 time: 9.7 s\n",
      "[2 100] loss: 0.476953 metric: 0.595506 time: 9.7 s\n",
      "[2 150] loss: 0.470129 metric: 0.534884 time: 10.3 s\n",
      "[2 200] loss: 0.497039 metric: 0.632353 time: 10.0 s\n",
      "[2 250] loss: 0.486813 metric: 0.625000 time: 10.1 s\n",
      "[2 300] loss: 0.481175 metric: 0.529915 time: 10.2 s\n",
      "[2 350] loss: 0.489637 metric: 0.557692 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.496458 acc: 0.816062 f1: 0.569536 time: 74.9 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.426019 metric: 0.650307 time: 9.8 s\n",
      "[3 100] loss: 0.411151 metric: 0.670659 time: 9.7 s\n",
      "[3 150] loss: 0.410940 metric: 0.534884 time: 10.3 s\n",
      "[3 200] loss: 0.449278 metric: 0.661290 time: 9.9 s\n",
      "[3 250] loss: 0.438676 metric: 0.650602 time: 10.2 s\n",
      "[3 300] loss: 0.439528 metric: 0.566372 time: 10.3 s\n",
      "[3 350] loss: 0.449297 metric: 0.607843 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.476420 acc: 0.808255 f1: 0.584654 time: 74.9 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.377283 metric: 0.687500 time: 9.7 s\n",
      "[4 100] loss: 0.369333 metric: 0.687117 time: 9.7 s\n",
      "[4 150] loss: 0.369294 metric: 0.617021 time: 10.3 s\n",
      "[4 200] loss: 0.407714 metric: 0.715447 time: 9.9 s\n",
      "[4 250] loss: 0.398390 metric: 0.658824 time: 10.1 s\n",
      "[4 300] loss: 0.401587 metric: 0.618182 time: 10.2 s\n",
      "[4 350] loss: 0.405103 metric: 0.622642 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.515982 acc: 0.814988 f1: 0.581457 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.332973 metric: 0.712500 time: 9.7 s\n",
      "[5 100] loss: 0.321762 metric: 0.720497 time: 9.7 s\n",
      "[5 150] loss: 0.325136 metric: 0.659091 time: 10.3 s\n",
      "[5 200] loss: 0.362428 metric: 0.746269 time: 9.9 s\n",
      "[5 250] loss: 0.349703 metric: 0.750000 time: 10.2 s\n",
      "[5 300] loss: 0.357766 metric: 0.631579 time: 10.2 s\n",
      "[5 350] loss: 0.355844 metric: 0.649573 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.612075 acc: 0.816257 f1: 0.570582 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.296936 metric: 0.720497 time: 9.7 s\n",
      "[6 100] loss: 0.276297 metric: 0.776316 time: 9.7 s\n",
      "[6 150] loss: 0.275762 metric: 0.729167 time: 10.3 s\n",
      "[6 200] loss: 0.319640 metric: 0.778626 time: 10.0 s\n",
      "[6 250] loss: 0.299823 metric: 0.808989 time: 10.1 s\n",
      "[6 300] loss: 0.313517 metric: 0.640000 time: 10.3 s\n",
      "[6 350] loss: 0.314473 metric: 0.736842 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.635155 acc: 0.771370 f1: 0.554817 time: 75.0 s\n",
      "************************************************************\n",
      "early stop at [6] epoch!\n",
      "fold  4\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.688281 metric: 0.374603 time: 9.9 s\n",
      "[1 100] loss: 0.670923 metric: 0.428571 time: 9.7 s\n",
      "[1 150] loss: 0.623801 metric: 0.250000 time: 10.3 s\n",
      "[1 200] loss: 0.609827 metric: 0.484472 time: 10.0 s\n",
      "[1 250] loss: 0.591093 metric: 0.574468 time: 10.3 s\n",
      "[1 300] loss: 0.566429 metric: 0.486486 time: 10.2 s\n",
      "[1 350] loss: 0.560700 metric: 0.479167 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.557096 acc: 0.794692 f1: 0.520510 time: 75.2 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.514258 metric: 0.588957 time: 9.9 s\n",
      "[2 100] loss: 0.486951 metric: 0.588889 time: 9.7 s\n",
      "[2 150] loss: 0.473208 metric: 0.595238 time: 10.3 s\n",
      "[2 200] loss: 0.494465 metric: 0.588235 time: 10.1 s\n",
      "[2 250] loss: 0.491298 metric: 0.574468 time: 10.3 s\n",
      "[2 300] loss: 0.481109 metric: 0.610687 time: 10.2 s\n",
      "[2 350] loss: 0.486373 metric: 0.485437 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.526347 acc: 0.808646 f1: 0.567013 time: 75.2 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.440261 metric: 0.649351 time: 9.9 s\n",
      "[3 100] loss: 0.414141 metric: 0.640000 time: 9.7 s\n",
      "[3 150] loss: 0.413584 metric: 0.588235 time: 10.3 s\n",
      "[3 200] loss: 0.445104 metric: 0.622222 time: 10.1 s\n",
      "[3 250] loss: 0.438661 metric: 0.608696 time: 10.2 s\n",
      "[3 300] loss: 0.440673 metric: 0.650794 time: 10.2 s\n",
      "[3 350] loss: 0.446617 metric: 0.537634 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.584777 acc: 0.826112 f1: 0.572046 time: 75.1 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.391907 metric: 0.666667 time: 9.9 s\n",
      "[4 100] loss: 0.366811 metric: 0.694118 time: 9.7 s\n",
      "[4 150] loss: 0.365237 metric: 0.576923 time: 10.3 s\n",
      "[4 200] loss: 0.402997 metric: 0.656716 time: 10.0 s\n",
      "[4 250] loss: 0.393515 metric: 0.645161 time: 10.3 s\n",
      "[4 300] loss: 0.398073 metric: 0.682171 time: 10.2 s\n",
      "[4 350] loss: 0.399244 metric: 0.631579 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.644219 acc: 0.827966 f1: 0.568632 time: 75.1 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.343622 metric: 0.666667 time: 9.9 s\n",
      "[5 100] loss: 0.323196 metric: 0.706587 time: 9.7 s\n",
      "[5 150] loss: 0.316767 metric: 0.640777 time: 10.2 s\n",
      "[5 200] loss: 0.356792 metric: 0.691729 time: 10.0 s\n",
      "[5 250] loss: 0.341627 metric: 0.622642 time: 10.3 s\n",
      "[5 300] loss: 0.350418 metric: 0.740157 time: 10.2 s\n",
      "[5 350] loss: 0.350612 metric: 0.719101 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.792376 acc: 0.826991 f1: 0.568088 time: 74.9 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.295832 metric: 0.690476 time: 9.9 s\n",
      "[6 100] loss: 0.278363 metric: 0.762500 time: 9.8 s\n",
      "[6 150] loss: 0.266551 metric: 0.718447 time: 10.2 s\n",
      "[6 200] loss: 0.307593 metric: 0.720588 time: 10.0 s\n",
      "[6 250] loss: 0.301511 metric: 0.818182 time: 10.3 s\n",
      "[6 300] loss: 0.312204 metric: 0.758065 time: 10.2 s\n",
      "[6 350] loss: 0.324084 metric: 0.686275 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.822785 acc: 0.819087 f1: 0.570635 time: 75.0 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[7 50] loss: 0.269046 metric: 0.716981 time: 9.8 s\n",
      "[7 100] loss: 0.249076 metric: 0.775758 time: 9.7 s\n",
      "[7 150] loss: 0.256733 metric: 0.714286 time: 10.2 s\n",
      "[7 200] loss: 0.290029 metric: 0.823529 time: 10.0 s\n",
      "[7 250] loss: 0.304030 metric: 0.764045 time: 10.2 s\n",
      "[7 300] loss: 0.315144 metric: 0.786325 time: 10.1 s\n",
      "[7 350] loss: 0.311128 metric: 0.725275 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 7]. loss: 0.647795 acc: 0.769516 f1: 0.560313 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[8 50] loss: 0.268215 metric: 0.723926 time: 9.9 s\n",
      "[8 100] loss: 0.227277 metric: 0.733333 time: 9.7 s\n",
      "[8 150] loss: 0.245922 metric: 0.622951 time: 10.2 s\n",
      "[8 200] loss: 0.272683 metric: 0.844828 time: 10.0 s\n",
      "[8 250] loss: 0.263225 metric: 0.699029 time: 10.2 s\n",
      "[8 300] loss: 0.300736 metric: 0.831858 time: 10.1 s\n",
      "[8 350] loss: 0.282727 metric: 0.642857 time: 10.3 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 8]. loss: 0.776658 acc: 0.802986 f1: 0.577880 time: 74.9 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[9 50] loss: 0.204824 metric: 0.737500 time: 9.9 s\n",
      "[9 100] loss: 0.196151 metric: 0.853333 time: 9.7 s\n",
      "[9 150] loss: 0.208127 metric: 0.804348 time: 10.2 s\n",
      "[9 200] loss: 0.238636 metric: 0.837607 time: 10.1 s\n",
      "[9 250] loss: 0.202998 metric: 0.692308 time: 10.2 s\n",
      "[9 300] loss: 0.266178 metric: 0.865385 time: 10.2 s\n",
      "[9 350] loss: 0.242108 metric: 0.626087 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 9]. loss: 1.243443 acc: 0.832260 f1: 0.542454 time: 75.0 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[10 50] loss: 0.191549 metric: 0.786667 time: 9.9 s\n",
      "[10 100] loss: 0.177998 metric: 0.835443 time: 9.6 s\n",
      "[10 150] loss: 0.159326 metric: 0.938272 time: 10.3 s\n",
      "[10 200] loss: 0.230709 metric: 0.694444 time: 10.1 s\n",
      "[10 250] loss: 0.203665 metric: 0.853659 time: 10.3 s\n",
      "[10 300] loss: 0.210273 metric: 0.886792 time: 10.2 s\n",
      "[10 350] loss: 0.220893 metric: 0.720000 time: 10.3 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 10]. loss: 1.509669 acc: 0.841725 f1: 0.510856 time: 75.1 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[11 50] loss: 0.176090 metric: 0.819444 time: 9.9 s\n",
      "[11 100] loss: 0.185764 metric: 0.814815 time: 9.7 s\n",
      "[11 150] loss: 0.152614 metric: 0.826087 time: 10.2 s\n",
      "[11 200] loss: 0.206126 metric: 0.867257 time: 10.1 s\n",
      "[11 250] loss: 0.161033 metric: 0.867470 time: 10.3 s\n",
      "[11 300] loss: 0.171558 metric: 0.903846 time: 10.2 s\n",
      "[11 350] loss: 0.199741 metric: 0.847059 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 11]. loss: 1.085927 acc: 0.816159 f1: 0.564695 time: 75.1 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[12 50] loss: 0.166406 metric: 0.867647 time: 9.9 s\n",
      "[12 100] loss: 0.208989 metric: 0.897059 time: 9.7 s\n",
      "[12 150] loss: 0.156703 metric: 0.915663 time: 10.2 s\n",
      "[12 200] loss: 0.157400 metric: 0.900901 time: 10.0 s\n",
      "[12 250] loss: 0.140875 metric: 0.808989 time: 10.2 s\n",
      "[12 300] loss: 0.148550 metric: 0.870370 time: 10.2 s\n",
      "[12 350] loss: 0.191894 metric: 0.757895 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 12]. loss: 1.396865 acc: 0.823868 f1: 0.545912 time: 74.9 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[13 50] loss: 0.138085 metric: 0.836879 time: 9.9 s\n",
      "[13 100] loss: 0.180279 metric: 0.914286 time: 9.7 s\n",
      "[13 150] loss: 0.158355 metric: 0.894118 time: 10.2 s\n",
      "[13 200] loss: 0.126865 metric: 0.847458 time: 10.0 s\n",
      "[13 250] loss: 0.132325 metric: 0.808989 time: 10.2 s\n",
      "[13 300] loss: 0.129467 metric: 0.803419 time: 10.2 s\n",
      "[13 350] loss: 0.215328 metric: 0.823529 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 13]. loss: 1.413387 acc: 0.828454 f1: 0.547141 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[14 50] loss: 0.131807 metric: 0.825175 time: 9.9 s\n",
      "[14 100] loss: 0.141109 metric: 0.909091 time: 9.6 s\n",
      "[14 150] loss: 0.132948 metric: 0.938272 time: 10.2 s\n",
      "[14 200] loss: 0.108754 metric: 0.909091 time: 10.0 s\n",
      "[14 250] loss: 0.133097 metric: 0.945946 time: 10.2 s\n",
      "[14 300] loss: 0.132191 metric: 0.870370 time: 10.1 s\n",
      "[14 350] loss: 0.196518 metric: 0.909091 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 14]. loss: 1.030654 acc: 0.800839 f1: 0.562299 time: 74.7 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[15 50] loss: 0.129770 metric: 0.899225 time: 9.8 s\n",
      "[15 100] loss: 0.124939 metric: 0.868421 time: 9.7 s\n",
      "[15 150] loss: 0.116241 metric: 0.974359 time: 10.2 s\n",
      "[15 200] loss: 0.085776 metric: 0.961538 time: 10.0 s\n",
      "[15 250] loss: 0.133834 metric: 0.947368 time: 10.3 s\n",
      "[15 300] loss: 0.142012 metric: 0.938776 time: 10.2 s\n",
      "[15 350] loss: 0.123455 metric: 0.867470 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 15]. loss: 1.374660 acc: 0.814305 f1: 0.557545 time: 74.9 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[16 50] loss: 0.106794 metric: 0.913386 time: 9.8 s\n",
      "[16 100] loss: 0.124061 metric: 0.904110 time: 9.7 s\n",
      "[16 150] loss: 0.097092 metric: 0.962025 time: 10.2 s\n",
      "[16 200] loss: 0.080366 metric: 0.961538 time: 10.0 s\n",
      "[16 250] loss: 0.102568 metric: 0.935065 time: 10.2 s\n",
      "[16 300] loss: 0.129476 metric: 0.948454 time: 10.1 s\n",
      "[16 350] loss: 0.091035 metric: 0.837209 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 16]. loss: 2.179533 acc: 0.835870 f1: 0.511898 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[17 50] loss: 0.077964 metric: 0.929134 time: 9.8 s\n",
      "[17 100] loss: 0.120145 metric: 0.929577 time: 9.6 s\n",
      "[17 150] loss: 0.072004 metric: 0.950000 time: 10.2 s\n",
      "[17 200] loss: 0.084965 metric: 0.980392 time: 10.0 s\n",
      "[17 250] loss: 0.073940 metric: 0.888889 time: 10.2 s\n",
      "[17 300] loss: 0.088719 metric: 0.989474 time: 10.1 s\n",
      "[17 350] loss: 0.068958 metric: 0.911392 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 17]. loss: 2.784395 acc: 0.837724 f1: 0.473234 time: 74.7 s\n",
      "************************************************************\n",
      "early stop at [17] epoch!\n",
      "fold  5\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.688906 metric: 0.389937 time: 9.6 s\n",
      "[1 100] loss: 0.684651 metric: 0.397727 time: 9.6 s\n",
      "[1 150] loss: 0.647445 metric: 0.226415 time: 10.5 s\n",
      "[1 200] loss: 0.631363 metric: 0.412698 time: 10.0 s\n",
      "[1 250] loss: 0.604729 metric: 0.462963 time: 10.2 s\n",
      "[1 300] loss: 0.578921 metric: 0.507937 time: 10.1 s\n",
      "[1 350] loss: 0.570067 metric: 0.481481 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.556883 acc: 0.787861 f1: 0.511899 time: 74.7 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.519450 metric: 0.625000 time: 9.6 s\n",
      "[2 100] loss: 0.490705 metric: 0.560976 time: 9.7 s\n",
      "[2 150] loss: 0.469022 metric: 0.583333 time: 10.5 s\n",
      "[2 200] loss: 0.498641 metric: 0.558824 time: 10.0 s\n",
      "[2 250] loss: 0.493992 metric: 0.636364 time: 10.2 s\n",
      "[2 300] loss: 0.487662 metric: 0.598291 time: 10.1 s\n",
      "[2 350] loss: 0.494305 metric: 0.541667 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.523855 acc: 0.819965 f1: 0.567814 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.435815 metric: 0.670659 time: 9.6 s\n",
      "[3 100] loss: 0.408812 metric: 0.590361 time: 9.7 s\n",
      "[3 150] loss: 0.407200 metric: 0.673684 time: 10.5 s\n",
      "[3 200] loss: 0.450869 metric: 0.631579 time: 10.0 s\n",
      "[3 250] loss: 0.443855 metric: 0.666667 time: 10.2 s\n",
      "[3 300] loss: 0.439943 metric: 0.672269 time: 10.1 s\n",
      "[3 350] loss: 0.451820 metric: 0.574468 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.531742 acc: 0.820453 f1: 0.571096 time: 74.7 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.388153 metric: 0.708075 time: 9.6 s\n",
      "[4 100] loss: 0.362239 metric: 0.638037 time: 9.7 s\n",
      "[4 150] loss: 0.361464 metric: 0.693878 time: 10.4 s\n",
      "[4 200] loss: 0.412002 metric: 0.661538 time: 10.0 s\n",
      "[4 250] loss: 0.397430 metric: 0.688172 time: 10.2 s\n",
      "[4 300] loss: 0.394052 metric: 0.700000 time: 10.1 s\n",
      "[4 350] loss: 0.408028 metric: 0.602151 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.573255 acc: 0.820550 f1: 0.571429 time: 74.7 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.340592 metric: 0.700000 time: 9.6 s\n",
      "[5 100] loss: 0.312270 metric: 0.686747 time: 9.7 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 150] loss: 0.319370 metric: 0.736842 time: 10.5 s\n",
      "[5 200] loss: 0.363603 metric: 0.750000 time: 10.0 s\n",
      "[5 250] loss: 0.340972 metric: 0.783505 time: 10.2 s\n",
      "[5 300] loss: 0.350628 metric: 0.741379 time: 10.1 s\n",
      "[5 350] loss: 0.358264 metric: 0.689655 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.668482 acc: 0.820746 f1: 0.565824 time: 74.7 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.295466 metric: 0.784314 time: 9.6 s\n",
      "[6 100] loss: 0.264316 metric: 0.772414 time: 9.7 s\n",
      "[6 150] loss: 0.285248 metric: 0.744681 time: 10.5 s\n",
      "[6 200] loss: 0.336778 metric: 0.720000 time: 10.0 s\n",
      "[6 250] loss: 0.307273 metric: 0.795699 time: 10.2 s\n",
      "[6 300] loss: 0.314175 metric: 0.784314 time: 10.1 s\n",
      "[6 350] loss: 0.337346 metric: 0.708333 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.880056 acc: 0.829723 f1: 0.556544 time: 74.7 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[7 50] loss: 0.268030 metric: 0.677596 time: 9.6 s\n",
      "[7 100] loss: 0.243665 metric: 0.670455 time: 9.7 s\n",
      "[7 150] loss: 0.264870 metric: 0.513158 time: 10.4 s\n",
      "[7 200] loss: 0.337756 metric: 0.765217 time: 10.0 s\n",
      "[7 250] loss: 0.269766 metric: 0.750000 time: 10.2 s\n",
      "[7 300] loss: 0.300989 metric: 0.682927 time: 10.1 s\n",
      "[7 350] loss: 0.319125 metric: 0.721649 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 7]. loss: 0.747422 acc: 0.790886 f1: 0.554933 time: 74.5 s\n",
      "************************************************************\n",
      "early stop at [7] epoch!\n",
      "fold  6\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.689574 metric: 0.369427 time: 9.6 s\n",
      "[1 100] loss: 0.682498 metric: 0.274510 time: 9.7 s\n",
      "[1 150] loss: 0.663273 metric: 0.256410 time: 10.5 s\n",
      "[1 200] loss: 0.644980 metric: 0.402299 time: 10.0 s\n",
      "[1 250] loss: 0.622114 metric: 0.425197 time: 10.2 s\n",
      "[1 300] loss: 0.588311 metric: 0.515152 time: 10.1 s\n",
      "[1 350] loss: 0.578076 metric: 0.433962 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.544134 acc: 0.750195 f1: 0.498039 time: 75.0 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.524402 metric: 0.611465 time: 9.6 s\n",
      "[2 100] loss: 0.498911 metric: 0.569536 time: 9.7 s\n",
      "[2 150] loss: 0.478559 metric: 0.533333 time: 10.5 s\n",
      "[2 200] loss: 0.502015 metric: 0.557823 time: 10.0 s\n",
      "[2 250] loss: 0.499161 metric: 0.595745 time: 10.2 s\n",
      "[2 300] loss: 0.491107 metric: 0.630631 time: 10.1 s\n",
      "[2 350] loss: 0.492626 metric: 0.605505 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.505999 acc: 0.817135 f1: 0.562966 time: 75.1 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.437626 metric: 0.666667 time: 9.6 s\n",
      "[3 100] loss: 0.416044 metric: 0.575000 time: 9.8 s\n",
      "[3 150] loss: 0.414378 metric: 0.574257 time: 10.5 s\n",
      "[3 200] loss: 0.450914 metric: 0.607407 time: 10.0 s\n",
      "[3 250] loss: 0.449186 metric: 0.639175 time: 10.2 s\n",
      "[3 300] loss: 0.449389 metric: 0.685714 time: 10.1 s\n",
      "[3 350] loss: 0.447542 metric: 0.654206 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.516343 acc: 0.821233 f1: 0.567312 time: 75.1 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.392753 metric: 0.710526 time: 9.6 s\n",
      "[4 100] loss: 0.373076 metric: 0.616438 time: 9.7 s\n",
      "[4 150] loss: 0.369397 metric: 0.588235 time: 10.5 s\n",
      "[4 200] loss: 0.413168 metric: 0.620155 time: 10.0 s\n",
      "[4 250] loss: 0.405632 metric: 0.617021 time: 10.3 s\n",
      "[4 300] loss: 0.407170 metric: 0.701754 time: 10.1 s\n",
      "[4 350] loss: 0.407622 metric: 0.672897 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.544015 acc: 0.820843 f1: 0.572028 time: 75.0 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.349957 metric: 0.741722 time: 9.6 s\n",
      "[5 100] loss: 0.331617 metric: 0.653061 time: 9.8 s\n",
      "[5 150] loss: 0.325006 metric: 0.613861 time: 10.5 s\n",
      "[5 200] loss: 0.370250 metric: 0.672000 time: 10.0 s\n",
      "[5 250] loss: 0.357362 metric: 0.744681 time: 10.2 s\n",
      "[5 300] loss: 0.363161 metric: 0.725664 time: 10.1 s\n",
      "[5 350] loss: 0.365322 metric: 0.698113 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.592117 acc: 0.811963 f1: 0.566674 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.306086 metric: 0.736842 time: 9.6 s\n",
      "[6 100] loss: 0.286220 metric: 0.757143 time: 9.7 s\n",
      "[6 150] loss: 0.281280 metric: 0.600000 time: 10.4 s\n",
      "[6 200] loss: 0.334027 metric: 0.765217 time: 10.0 s\n",
      "[6 250] loss: 0.310497 metric: 0.765957 time: 10.2 s\n",
      "[6 300] loss: 0.318137 metric: 0.800000 time: 10.1 s\n",
      "[6 350] loss: 0.316090 metric: 0.740741 time: 10.3 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.864909 acc: 0.833041 f1: 0.555930 time: 75.0 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[7 50] loss: 0.270183 metric: 0.734177 time: 9.6 s\n",
      "[7 100] loss: 0.268109 metric: 0.692308 time: 9.8 s\n",
      "[7 150] loss: 0.268880 metric: 0.717391 time: 10.5 s\n",
      "[7 200] loss: 0.304918 metric: 0.741935 time: 10.0 s\n",
      "[7 250] loss: 0.288724 metric: 0.795918 time: 10.2 s\n",
      "[7 300] loss: 0.326977 metric: 0.733333 time: 10.2 s\n",
      "[7 350] loss: 0.289408 metric: 0.725664 time: 10.3 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 7]. loss: 0.861206 acc: 0.827771 f1: 0.555304 time: 75.1 s\n",
      "************************************************************\n",
      "early stop at [7] epoch!\n",
      "fold  7\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.688029 metric: 0.358974 time: 9.8 s\n",
      "[1 100] loss: 0.678143 metric: 0.468293 time: 9.9 s\n",
      "[1 150] loss: 0.631847 metric: 0.419355 time: 10.2 s\n",
      "[1 200] loss: 0.611625 metric: 0.465116 time: 9.9 s\n",
      "[1 250] loss: 0.585628 metric: 0.453782 time: 10.3 s\n",
      "[1 300] loss: 0.562439 metric: 0.461538 time: 10.2 s\n",
      "[1 350] loss: 0.560143 metric: 0.454545 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.543797 acc: 0.742584 f1: 0.510938 time: 75.0 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.508181 metric: 0.644295 time: 9.9 s\n",
      "[2 100] loss: 0.478697 metric: 0.600000 time: 9.9 s\n",
      "[2 150] loss: 0.465777 metric: 0.554455 time: 10.2 s\n",
      "[2 200] loss: 0.494371 metric: 0.614379 time: 9.8 s\n",
      "[2 250] loss: 0.485219 metric: 0.637363 time: 10.3 s\n",
      "[2 300] loss: 0.482343 metric: 0.512821 time: 10.2 s\n",
      "[2 350] loss: 0.486493 metric: 0.545455 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.526728 acc: 0.800449 f1: 0.565541 time: 74.9 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.436099 metric: 0.676056 time: 9.8 s\n",
      "[3 100] loss: 0.408091 metric: 0.643275 time: 9.9 s\n",
      "[3 150] loss: 0.408911 metric: 0.596154 time: 10.1 s\n",
      "[3 200] loss: 0.446405 metric: 0.666667 time: 9.8 s\n",
      "[3 250] loss: 0.434760 metric: 0.674699 time: 10.3 s\n",
      "[3 300] loss: 0.444656 metric: 0.556522 time: 10.1 s\n",
      "[3 350] loss: 0.445958 metric: 0.559140 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.538908 acc: 0.812256 f1: 0.573203 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.388310 metric: 0.679739 time: 9.8 s\n",
      "[4 100] loss: 0.364610 metric: 0.720497 time: 9.9 s\n",
      "[4 150] loss: 0.368232 metric: 0.593220 time: 10.2 s\n",
      "[4 200] loss: 0.407763 metric: 0.721805 time: 9.8 s\n",
      "[4 250] loss: 0.388120 metric: 0.720930 time: 10.3 s\n",
      "[4 300] loss: 0.399459 metric: 0.554622 time: 10.1 s\n",
      "[4 350] loss: 0.403871 metric: 0.640777 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.578782 acc: 0.816159 f1: 0.577957 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.340859 metric: 0.687898 time: 9.8 s\n",
      "[5 100] loss: 0.320621 metric: 0.751515 time: 9.9 s\n",
      "[5 150] loss: 0.327959 metric: 0.600000 time: 10.1 s\n",
      "[5 200] loss: 0.366174 metric: 0.732824 time: 9.8 s\n",
      "[5 250] loss: 0.334119 metric: 0.814815 time: 10.2 s\n",
      "[5 300] loss: 0.352641 metric: 0.635514 time: 10.2 s\n",
      "[5 350] loss: 0.355397 metric: 0.736842 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.673339 acc: 0.819965 f1: 0.568219 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.293179 metric: 0.718954 time: 9.8 s\n",
      "[6 100] loss: 0.277955 metric: 0.782051 time: 9.9 s\n",
      "[6 150] loss: 0.301120 metric: 0.550725 time: 10.2 s\n",
      "[6 200] loss: 0.358114 metric: 0.766667 time: 9.8 s\n",
      "[6 250] loss: 0.289423 metric: 0.829268 time: 10.3 s\n",
      "[6 300] loss: 0.323441 metric: 0.686275 time: 10.2 s\n",
      "[6 350] loss: 0.334655 metric: 0.727273 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.715655 acc: 0.808158 f1: 0.562528 time: 74.9 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[7 50] loss: 0.276938 metric: 0.769231 time: 9.8 s\n",
      "[7 100] loss: 0.239722 metric: 0.792453 time: 9.9 s\n",
      "[7 150] loss: 0.297673 metric: 0.733333 time: 10.1 s\n",
      "[7 200] loss: 0.336045 metric: 0.813559 time: 9.8 s\n",
      "[7 250] loss: 0.266535 metric: 0.800000 time: 10.2 s\n",
      "[7 300] loss: 0.342357 metric: 0.593220 time: 10.1 s\n",
      "[7 350] loss: 0.281627 metric: 0.765957 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 7]. loss: 0.630706 acc: 0.728142 f1: 0.526513 time: 74.6 s\n",
      "************************************************************\n",
      "early stop at [7] epoch!\n",
      "fold  8\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.689195 metric: 0.384858 time: 9.8 s\n",
      "[1 100] loss: 0.684230 metric: 0.392344 time: 9.8 s\n",
      "[1 150] loss: 0.657775 metric: 0.337662 time: 10.3 s\n",
      "[1 200] loss: 0.639311 metric: 0.403941 time: 10.0 s\n",
      "[1 250] loss: 0.615212 metric: 0.446429 time: 10.2 s\n",
      "[1 300] loss: 0.586317 metric: 0.500000 time: 10.1 s\n",
      "[1 350] loss: 0.583855 metric: 0.489362 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.557528 acc: 0.796916 f1: 0.506755 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.519601 metric: 0.684932 time: 9.8 s\n",
      "[2 100] loss: 0.485337 metric: 0.636872 time: 9.8 s\n",
      "[2 150] loss: 0.477648 metric: 0.566038 time: 10.2 s\n",
      "[2 200] loss: 0.504444 metric: 0.578616 time: 10.0 s\n",
      "[2 250] loss: 0.494612 metric: 0.606742 time: 10.2 s\n",
      "[2 300] loss: 0.482393 metric: 0.569106 time: 10.1 s\n",
      "[2 350] loss: 0.491225 metric: 0.518519 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.501455 acc: 0.808139 f1: 0.559982 time: 74.7 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.435761 metric: 0.654088 time: 9.8 s\n",
      "[3 100] loss: 0.411249 metric: 0.686047 time: 9.8 s\n",
      "[3 150] loss: 0.416020 metric: 0.589286 time: 10.3 s\n",
      "[3 200] loss: 0.448594 metric: 0.620690 time: 10.0 s\n",
      "[3 250] loss: 0.446727 metric: 0.644444 time: 10.2 s\n",
      "[3 300] loss: 0.437467 metric: 0.616667 time: 10.1 s\n",
      "[3 350] loss: 0.449449 metric: 0.585859 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.520085 acc: 0.822777 f1: 0.570888 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.387583 metric: 0.662500 time: 9.8 s\n",
      "[4 100] loss: 0.363292 metric: 0.723926 time: 9.8 s\n",
      "[4 150] loss: 0.376791 metric: 0.593220 time: 10.3 s\n",
      "[4 200] loss: 0.409420 metric: 0.691729 time: 10.0 s\n",
      "[4 250] loss: 0.405281 metric: 0.666667 time: 10.2 s\n",
      "[4 300] loss: 0.398364 metric: 0.713043 time: 10.1 s\n",
      "[4 350] loss: 0.414111 metric: 0.618557 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.555940 acc: 0.823656 f1: 0.572105 time: 74.8 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.341832 metric: 0.687117 time: 9.9 s\n",
      "[5 100] loss: 0.317977 metric: 0.772152 time: 9.8 s\n",
      "[5 150] loss: 0.336997 metric: 0.580153 time: 10.3 s\n",
      "[5 200] loss: 0.367859 metric: 0.750000 time: 10.0 s\n",
      "[5 250] loss: 0.361763 metric: 0.733333 time: 10.2 s\n",
      "[5 300] loss: 0.363355 metric: 0.725664 time: 10.1 s\n",
      "[5 350] loss: 0.378998 metric: 0.659341 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.566076 acc: 0.807846 f1: 0.569241 time: 75.1 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.302189 metric: 0.705202 time: 9.8 s\n",
      "[6 100] loss: 0.271889 metric: 0.817610 time: 9.8 s\n",
      "[6 150] loss: 0.319318 metric: 0.733333 time: 10.3 s\n",
      "[6 200] loss: 0.332440 metric: 0.736842 time: 9.9 s\n",
      "[6 250] loss: 0.317493 metric: 0.792079 time: 10.2 s\n",
      "[6 300] loss: 0.359358 metric: 0.638298 time: 10.1 s\n",
      "[6 350] loss: 0.341286 metric: 0.631579 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.539155 acc: 0.742071 f1: 0.532791 time: 74.9 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[7 50] loss: 0.282740 metric: 0.774194 time: 9.8 s\n",
      "[7 100] loss: 0.245615 metric: 0.740741 time: 9.8 s\n",
      "[7 150] loss: 0.278110 metric: 0.737864 time: 10.3 s\n",
      "[7 200] loss: 0.291800 metric: 0.724638 time: 10.0 s\n",
      "[7 250] loss: 0.304826 metric: 0.761905 time: 10.2 s\n",
      "[7 300] loss: 0.325795 metric: 0.759259 time: 10.1 s\n",
      "[7 350] loss: 0.288954 metric: 0.648649 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 7]. loss: 0.670599 acc: 0.786572 f1: 0.552669 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[8 50] loss: 0.248731 metric: 0.805369 time: 9.8 s\n",
      "[8 100] loss: 0.219417 metric: 0.809249 time: 9.7 s\n",
      "[8 150] loss: 0.225910 metric: 0.757282 time: 10.3 s\n",
      "[8 200] loss: 0.252325 metric: 0.781250 time: 10.0 s\n",
      "[8 250] loss: 0.268116 metric: 0.733945 time: 10.2 s\n",
      "[8 300] loss: 0.301262 metric: 0.774775 time: 10.1 s\n",
      "[8 350] loss: 0.252650 metric: 0.795455 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 8]. loss: 1.176531 acc: 0.836830 f1: 0.550054 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[9 50] loss: 0.243103 metric: 0.824324 time: 9.8 s\n",
      "[9 100] loss: 0.216201 metric: 0.790960 time: 9.8 s\n",
      "[9 150] loss: 0.194656 metric: 0.757282 time: 10.3 s\n",
      "[9 200] loss: 0.241967 metric: 0.821429 time: 10.0 s\n",
      "[9 250] loss: 0.243962 metric: 0.666667 time: 10.2 s\n",
      "[9 300] loss: 0.263842 metric: 0.743802 time: 10.1 s\n",
      "[9 350] loss: 0.218323 metric: 0.808989 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 9]. loss: 0.915072 acc: 0.799258 f1: 0.556967 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[10 50] loss: 0.203062 metric: 0.777070 time: 9.9 s\n",
      "[10 100] loss: 0.196084 metric: 0.813953 time: 9.8 s\n",
      "[10 150] loss: 0.178948 metric: 0.928571 time: 10.3 s\n",
      "[10 200] loss: 0.230297 metric: 0.813008 time: 10.0 s\n",
      "[10 250] loss: 0.207846 metric: 0.792079 time: 10.2 s\n",
      "[10 300] loss: 0.233507 metric: 0.833333 time: 10.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 350] loss: 0.198630 metric: 0.795699 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 10]. loss: 0.780588 acc: 0.750073 f1: 0.532579 time: 74.9 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[11 50] loss: 0.194942 metric: 0.777070 time: 9.8 s\n",
      "[11 100] loss: 0.230831 metric: 0.811594 time: 9.7 s\n",
      "[11 150] loss: 0.190757 metric: 0.886364 time: 10.3 s\n",
      "[11 200] loss: 0.189501 metric: 0.847458 time: 10.0 s\n",
      "[11 250] loss: 0.178154 metric: 0.930233 time: 10.2 s\n",
      "[11 300] loss: 0.198043 metric: 0.891089 time: 10.1 s\n",
      "[11 350] loss: 0.179800 metric: 0.672727 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 11]. loss: 1.276284 acc: 0.838684 f1: 0.566256 time: 74.6 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[12 50] loss: 0.185847 metric: 0.916031 time: 9.8 s\n",
      "[12 100] loss: 0.207054 metric: 0.853659 time: 9.8 s\n",
      "[12 150] loss: 0.145176 metric: 0.917647 time: 10.2 s\n",
      "[12 200] loss: 0.160321 metric: 0.869565 time: 9.9 s\n",
      "[12 250] loss: 0.185328 metric: 0.962963 time: 10.1 s\n",
      "[12 300] loss: 0.201235 metric: 0.857143 time: 10.1 s\n",
      "[12 350] loss: 0.141524 metric: 0.755102 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 12]. loss: 2.091101 acc: 0.847663 f1: 0.463389 time: 74.5 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[13 50] loss: 0.173797 metric: 0.924242 time: 9.8 s\n",
      "[13 100] loss: 0.138507 metric: 0.864198 time: 9.7 s\n",
      "[13 150] loss: 0.117183 metric: 0.939759 time: 10.3 s\n",
      "[13 200] loss: 0.119620 metric: 0.934579 time: 10.0 s\n",
      "[13 250] loss: 0.146382 metric: 0.975610 time: 10.2 s\n",
      "[13 300] loss: 0.177934 metric: 0.882353 time: 10.1 s\n",
      "[13 350] loss: 0.113231 metric: 0.891566 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 13]. loss: 1.923406 acc: 0.845223 f1: 0.519102 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[14 50] loss: 0.153671 metric: 0.897059 time: 9.9 s\n",
      "[14 100] loss: 0.100041 metric: 0.880503 time: 9.8 s\n",
      "[14 150] loss: 0.102459 metric: 0.951220 time: 10.3 s\n",
      "[14 200] loss: 0.113087 metric: 0.943396 time: 9.9 s\n",
      "[14 250] loss: 0.129258 metric: 0.963855 time: 10.2 s\n",
      "[14 300] loss: 0.184349 metric: 0.891089 time: 10.1 s\n",
      "[14 350] loss: 0.094111 metric: 0.948718 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 14]. loss: 1.436031 acc: 0.822777 f1: 0.548483 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[15 50] loss: 0.137475 metric: 0.931298 time: 9.8 s\n",
      "[15 100] loss: 0.094578 metric: 0.915033 time: 9.8 s\n",
      "[15 150] loss: 0.095334 metric: 0.962963 time: 10.3 s\n",
      "[15 200] loss: 0.106362 metric: 0.952381 time: 10.0 s\n",
      "[15 250] loss: 0.121050 metric: 0.879121 time: 10.2 s\n",
      "[15 300] loss: 0.144877 metric: 0.916667 time: 10.1 s\n",
      "[15 350] loss: 0.090194 metric: 0.973684 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 15]. loss: 1.374212 acc: 0.799258 f1: 0.548210 time: 74.9 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[16 50] loss: 0.141504 metric: 0.952381 time: 9.8 s\n",
      "[16 100] loss: 0.094631 metric: 0.927152 time: 9.8 s\n",
      "[16 150] loss: 0.083879 metric: 0.962963 time: 10.3 s\n",
      "[16 200] loss: 0.089344 metric: 0.961538 time: 10.0 s\n",
      "[16 250] loss: 0.117458 metric: 0.930233 time: 10.2 s\n",
      "[16 300] loss: 0.121137 metric: 0.918367 time: 10.1 s\n",
      "[16 350] loss: 0.087861 metric: 1.000000 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 16]. loss: 1.671934 acc: 0.819655 f1: 0.538922 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[17 50] loss: 0.124052 metric: 0.976000 time: 9.8 s\n",
      "[17 100] loss: 0.095382 metric: 0.945946 time: 9.8 s\n",
      "[17 150] loss: 0.072075 metric: 0.975000 time: 10.3 s\n",
      "[17 200] loss: 0.062946 metric: 0.925926 time: 10.0 s\n",
      "[17 250] loss: 0.088702 metric: 1.000000 time: 10.2 s\n",
      "[17 300] loss: 0.096968 metric: 0.927835 time: 10.1 s\n",
      "[17 350] loss: 0.089969 metric: 0.986667 time: 10.0 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 17]. loss: 1.947979 acc: 0.831756 f1: 0.538544 time: 74.8 s\n",
      "************************************************************\n",
      "early stop at [17] epoch!\n",
      "fold  9\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.684657 metric: 0.367647 time: 9.7 s\n",
      "[1 100] loss: 0.667989 metric: 0.350649 time: 9.7 s\n",
      "[1 150] loss: 0.629890 metric: 0.433735 time: 10.3 s\n",
      "[1 200] loss: 0.606689 metric: 0.491429 time: 10.0 s\n",
      "[1 250] loss: 0.582829 metric: 0.454545 time: 10.1 s\n",
      "[1 300] loss: 0.552962 metric: 0.496000 time: 10.1 s\n",
      "[1 350] loss: 0.556934 metric: 0.486486 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.514217 acc: 0.741778 f1: 0.515562 time: 74.9 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.502508 metric: 0.602273 time: 9.8 s\n",
      "[2 100] loss: 0.469066 metric: 0.587500 time: 9.8 s\n",
      "[2 150] loss: 0.471416 metric: 0.574257 time: 10.4 s\n",
      "[2 200] loss: 0.489640 metric: 0.632353 time: 10.0 s\n",
      "[2 250] loss: 0.483343 metric: 0.597403 time: 10.2 s\n",
      "[2 300] loss: 0.477000 metric: 0.620690 time: 10.2 s\n",
      "[2 350] loss: 0.495504 metric: 0.565657 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.499633 acc: 0.812530 f1: 0.559505 time: 75.2 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.431097 metric: 0.684564 time: 9.8 s\n",
      "[3 100] loss: 0.408347 metric: 0.654762 time: 9.7 s\n",
      "[3 150] loss: 0.413896 metric: 0.582524 time: 10.4 s\n",
      "[3 200] loss: 0.443564 metric: 0.677165 time: 10.0 s\n",
      "[3 250] loss: 0.436840 metric: 0.675325 time: 10.1 s\n",
      "[3 300] loss: 0.436079 metric: 0.666667 time: 10.1 s\n",
      "[3 350] loss: 0.459632 metric: 0.633663 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.485656 acc: 0.799356 f1: 0.566976 time: 75.0 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.386512 metric: 0.716981 time: 9.8 s\n",
      "[4 100] loss: 0.369298 metric: 0.647727 time: 9.7 s\n",
      "[4 150] loss: 0.371416 metric: 0.582524 time: 10.4 s\n",
      "[4 200] loss: 0.404950 metric: 0.686567 time: 10.0 s\n",
      "[4 250] loss: 0.390595 metric: 0.780488 time: 10.1 s\n",
      "[4 300] loss: 0.391100 metric: 0.695652 time: 10.2 s\n",
      "[4 350] loss: 0.411032 metric: 0.629630 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.524076 acc: 0.800332 f1: 0.561884 time: 75.0 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.348664 metric: 0.729560 time: 9.7 s\n",
      "[5 100] loss: 0.325576 metric: 0.636364 time: 9.7 s\n",
      "[5 150] loss: 0.329910 metric: 0.553571 time: 10.3 s\n",
      "[5 200] loss: 0.363562 metric: 0.739130 time: 10.0 s\n",
      "[5 250] loss: 0.335926 metric: 0.781609 time: 10.1 s\n",
      "[5 300] loss: 0.348873 metric: 0.777778 time: 10.1 s\n",
      "[5 350] loss: 0.364911 metric: 0.685185 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.694996 acc: 0.825998 f1: 0.556798 time: 74.8 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.327039 metric: 0.767123 time: 9.8 s\n",
      "[6 100] loss: 0.287344 metric: 0.701754 time: 9.7 s\n",
      "[6 150] loss: 0.297961 metric: 0.666667 time: 10.3 s\n",
      "[6 200] loss: 0.326983 metric: 0.793388 time: 10.0 s\n",
      "[6 250] loss: 0.290491 metric: 0.843373 time: 10.2 s\n",
      "[6 300] loss: 0.317560 metric: 0.745455 time: 10.2 s\n",
      "[6 350] loss: 0.359127 metric: 0.709091 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.640703 acc: 0.791158 f1: 0.553609 time: 74.9 s\n",
      "************************************************************\n",
      "early stop at [6] epoch!\n",
      "fold  10\n",
      "Load embedding finished.\n",
      "Total words count: 1517, oov count: 8.\n",
      "Building vocabulary Finished.\n",
      "prepare data done!\n",
      "start train...\n",
      "learning rate: 0.000950\n",
      "[1 50] loss: 0.688464 metric: 0.364217 time: 9.8 s\n",
      "[1 100] loss: 0.683483 metric: 0.411290 time: 9.9 s\n",
      "[1 150] loss: 0.660599 metric: 0.376238 time: 10.3 s\n",
      "[1 200] loss: 0.645304 metric: 0.430556 time: 9.9 s\n",
      "[1 250] loss: 0.621070 metric: 0.464286 time: 10.3 s\n",
      "[1 300] loss: 0.584824 metric: 0.473684 time: 10.2 s\n",
      "[1 350] loss: 0.574994 metric: 0.454545 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 1]. loss: 0.548590 acc: 0.779350 f1: 0.507300 time: 75.1 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[2 50] loss: 0.522153 metric: 0.628571 time: 9.8 s\n",
      "[2 100] loss: 0.492193 metric: 0.554974 time: 9.9 s\n",
      "[2 150] loss: 0.477585 metric: 0.580000 time: 10.2 s\n",
      "[2 200] loss: 0.501212 metric: 0.583942 time: 9.9 s\n",
      "[2 250] loss: 0.495544 metric: 0.634615 time: 10.3 s\n",
      "[2 300] loss: 0.492101 metric: 0.571429 time: 10.2 s\n",
      "[2 350] loss: 0.494072 metric: 0.606742 time: 10.1 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 2]. loss: 0.508706 acc: 0.800039 f1: 0.556205 time: 75.1 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[3 50] loss: 0.437078 metric: 0.713287 time: 9.8 s\n",
      "[3 100] loss: 0.412366 metric: 0.635838 time: 9.9 s\n",
      "[3 150] loss: 0.417139 metric: 0.581197 time: 10.3 s\n",
      "[3 200] loss: 0.449375 metric: 0.600000 time: 10.0 s\n",
      "[3 250] loss: 0.447329 metric: 0.641509 time: 10.3 s\n",
      "[3 300] loss: 0.449971 metric: 0.593220 time: 10.2 s\n",
      "[3 350] loss: 0.449134 metric: 0.597938 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 3]. loss: 0.539761 acc: 0.819557 f1: 0.571892 time: 75.1 s\n",
      "************************************************************\n",
      "save model...\n",
      "learning rate: 0.000950\n",
      "[4 50] loss: 0.390670 metric: 0.757143 time: 9.8 s\n",
      "[4 100] loss: 0.368362 metric: 0.658824 time: 9.9 s\n",
      "[4 150] loss: 0.378034 metric: 0.578512 time: 10.3 s\n",
      "[4 200] loss: 0.405891 metric: 0.636364 time: 9.9 s\n",
      "[4 250] loss: 0.405564 metric: 0.618182 time: 10.3 s\n",
      "[4 300] loss: 0.409552 metric: 0.660870 time: 10.2 s\n",
      "[4 350] loss: 0.406100 metric: 0.653061 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 4]. loss: 0.592975 acc: 0.815849 f1: 0.564706 time: 75.2 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[5 50] loss: 0.350436 metric: 0.797101 time: 9.8 s\n",
      "[5 100] loss: 0.324059 metric: 0.695652 time: 9.9 s\n",
      "[5 150] loss: 0.333332 metric: 0.621849 time: 10.3 s\n",
      "[5 200] loss: 0.363846 metric: 0.661290 time: 10.0 s\n",
      "[5 250] loss: 0.362879 metric: 0.691589 time: 10.3 s\n",
      "[5 300] loss: 0.367895 metric: 0.678261 time: 10.1 s\n",
      "[5 350] loss: 0.357199 metric: 0.653846 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 5]. loss: 0.691772 acc: 0.818483 f1: 0.563585 time: 75.1 s\n",
      "************************************************************\n",
      "learning rate: 0.000950\n",
      "[6 50] loss: 0.308466 metric: 0.805755 time: 9.8 s\n",
      "[6 100] loss: 0.275378 metric: 0.742138 time: 9.9 s\n",
      "[6 150] loss: 0.286185 metric: 0.661017 time: 10.2 s\n",
      "[6 200] loss: 0.319170 metric: 0.735043 time: 10.0 s\n",
      "[6 250] loss: 0.322325 metric: 0.677966 time: 10.3 s\n",
      "[6 300] loss: 0.353292 metric: 0.635659 time: 10.2 s\n",
      "[6 350] loss: 0.335096 metric: 0.752941 time: 10.2 s\n",
      "Evaluating....\n",
      "************************************************************\n",
      "[epoch 6]. loss: 0.791607 acc: 0.814092 f1: 0.549965 time: 75.2 s\n",
      "************************************************************\n",
      "early stop at [6] epoch!\n"
     ]
    }
   ],
   "source": [
    "# 10折\n",
    "for i in range(1, 11):\n",
    "    print('fold ', i)\n",
    "    TEXT = data.Field(sequential=True, use_vocab=True, eos_token='<EOS>', init_token='<BOS>',pad_token='<PAD>', \n",
    "                      batch_first=True, tokenize=tokenizer)\n",
    "    LABEL = data.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "    tv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                     (\"txt1\", TEXT), (\"txt2\", TEXT),\n",
    "                     (\"label\", LABEL)]\n",
    "    train = data.TabularDataset(path='../datasets/KFold/train_fold_'+str(i)+'.csv', format='csv', skip_header=True, fields=tv_datafields)\n",
    "    valid = data.TabularDataset(path='../datasets/KFold/valid_fold_'+str(i)+'.csv', format='csv', skip_header=True, fields=tv_datafields)\n",
    "    TEXT.build_vocab(train, valid, min_freq=3)\n",
    "    matrix = wordlist_to_matrix('../datasets/pretrain_embedding/pretrain_full_emb.txt', TEXT.vocab.itos, DEVICE)\n",
    "    print_flush('Building vocabulary Finished.')\n",
    "    train_iter = data.BucketIterator(dataset=train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.txt1) + len(x.txt2), shuffle=False, device=-1, repeat=False)\n",
    "    valid_iter = data.Iterator(dataset=valid, batch_size=BATCH_SIZE, device=-1, shuffle=False, repeat=False)\n",
    "    train_dl = BatchWrapper(train_iter, [\"txt1\", \"txt2\", \"label\"])\n",
    "    valid_dl = BatchWrapper(valid_iter, [\"txt1\", \"txt2\", \"label\"])\n",
    "    print_flush('prepare data done!')\n",
    "    model = LSTM_INTERACTIVE_ATTENTION(TEXT.vocab, EMBED_DIM, HIDDEN_DIM, DEEP_LAYERS, matrix)\n",
    "    model.to(DEVICE)\n",
    "    parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    criterion = nn.NLLLoss(weight=torch.tensor(CLASS_WEIGHT).float().to(DEVICE))\n",
    "    optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "    \n",
    "    print_every = 50\n",
    "    best_state = None\n",
    "    max_metric = 0\n",
    "    valid_result = []\n",
    "    valid_iter.create_batches()\n",
    "    valid_batch_num = len(list(valid_iter.batches))\n",
    "    print_flush('start train...')\n",
    "    for epoch in range(EPOCHES):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate: %.6f'% param_group['lr'])\n",
    "        epoch_begin = time()\n",
    "        total_loss = 0.0\n",
    "        train_iter.init_epoch()\n",
    "        batch_count = 0\n",
    "        batch_begin_time = time()\n",
    "        for text1, text2, label in train_dl:\n",
    "            model.train()\n",
    "            text1 = text1.to(DEVICE)\n",
    "            text2 = text2.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            x1_mask, x2_mask = get_mask(text1, text2)\n",
    "            y_pred = model(text1, text2, x1_mask, x2_mask)\n",
    "            loss = criterion(y_pred, label)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            if batch_count % print_every == 0:\n",
    "                metric = evaluate(model.eval(), text1, text2, x1_mask, x2_mask, label)\n",
    "                print_flush('[%d %d] loss: %.6f metric: %.6f time: %.1f s' %\n",
    "                      (epoch + 1, batch_count, total_loss / print_every, metric, time() - batch_begin_time))\n",
    "                total_loss = 0.0\n",
    "                batch_begin_time = time()\n",
    "    #     scheduler.step()\n",
    "        print_flush(\"Evaluating....\")\n",
    "        loss, (acc, Precision, Recall, F1) = predict(model, valid_dl, criterion, DEVICE)\n",
    "        valid_result.append(F1)\n",
    "        print_flush('*'*60)\n",
    "        print_flush('[epoch %d]. loss: %.6f acc: %.6f f1: %.6f time: %.1f s'%(epoch+1, loss/valid_batch_num, acc, F1, time()-epoch_begin))\n",
    "        print_flush('*'*60)\n",
    "        if F1 > max_metric:\n",
    "            best_state = model.state_dict()\n",
    "            max_metric = F1\n",
    "            print_flush(\"save model...\")\n",
    "            torch.save(best_state, '../datasets/models/KFold/ESIM_fold_'+str(i)+'%.4f'%(F1)+'.pth')\n",
    "        epoch_begin = time()\n",
    "        if training_termination(valid_result):\n",
    "            print_flush(\"early stop at [%d] epoch!\" % (epoch+1))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
